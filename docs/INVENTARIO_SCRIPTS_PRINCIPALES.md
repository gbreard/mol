# üìã INVENTARIO DE SCRIPTS PRINCIPALES - MOL

**Proyecto:** Monitor de Ofertas Laborales (MOL)
**Total scripts en proyecto:** 248 (213 Python + 35 R)
**Scripts cr√≠ticos documentados:** 20
**Fecha:** 14/11/2025

---

## üìä TABLA RESUMEN

| # | Script | Grupo | Prop√≥sito | Frecuencia |
|---|--------|-------|-----------|------------|
| 1 | run_full_pipeline.py | Pipeline Maestro | Orquestador completo scraping multi-fuente | Manual/Programada |
| 2 | run_scheduler.py | Automatizaci√≥n | Scheduler 2x/semana (lunes y jueves 8 AM) | Loop continuo |
| 3 | bumeran_scraper.py | Scraping | Scraper API REST Bumeran (~12K ofertas) | Pipeline/Scheduler |
| 4 | scrapear_con_diccionario.py | Scraping | Multi-keyword Bumeran (m√°xima cobertura) | Incremental/Full |
| 5 | computrabajo_scraper.py | Scraping | Scraper HTML ComputRabajo (~500-1K) | Pipeline |
| 6 | linkedin_scraper.py | Scraping | Scraper JobSpy LinkedIn multi-keyword | Pipeline |
| 7 | zonajobs_scraper_final.py | Scraping | Scraper API REST ZonaJobs (~5K) | Pipeline |
| 8 | consolidar_fuentes.py | Consolidaci√≥n | Consolida multi-fuente a schema √∫nico | Post-scraping |
| 9 | normalizar_campos.py | Consolidaci√≥n | Normalizadores por fuente (40+ campos) | Consolidaci√≥n |
| 10 | incremental_tracker.py | Utilidad | Tracking IDs scrapeados (evita duplicados) | Cada scraping |
| 11 | run_nlp_extraction.py | NLP | Ejecutor NLP datasets completos | Manual post-consolidaci√≥n |
| 12 | bumeran_extractor.py | NLP | Extractor NLP Bumeran (regex patterns v3) | run_nlp_extraction |
| 13 | regex_patterns_v3.py | NLP | Patrones regex optimizados extracci√≥n | Extractores |
| 14 | esco_hybrid_matcher.py | ESCO | Matcher Fuzzy + Embeddings + LLM | Manual clasificaci√≥n |
| 15 | integracion_esco_semantica.py | ESCO | Integraci√≥n ofertas NLP + ESCO + skills | Post-NLP |
| 16 | db_manager.py | Base de Datos | Gestor SQLite dual-write schema v2 | Pipeline/Scheduler |
| 17 | init_sqlite.py | Base de Datos | Inicializador database SQLite | Setup inicial |
| 18 | populate_esco_from_rdf.py | Base de Datos | Parser ESCO RDF (3K ocupaciones + 13K skills) | Setup inicial |
| 19 | app.R | Dashboards | Dashboard Shiny an√°lisis mercado laboral | On-demand |
| 20 | dashboard_scraping_v4.py | Dashboards | Dashboard Python monitoreo scraping | On-demand |

---

## 1Ô∏è‚É£ PIPELINE MAESTRO

### 1. run_full_pipeline.py
**Ruta:** `D:\OEDE\Webscrapping\run_full_pipeline.py`

**Prop√≥sito:**
Orquestador principal que ejecuta el pipeline completo de scraping multi-fuente (Bumeran, ComputRabajo, ZonaJobs, LinkedIn, Indeed) + consolidaci√≥n.

**Inputs:**
- Diccionario de keywords (data/config/keywords_loader.py)
- Configuraci√≥n de fuentes (01_sources/)

**Outputs:**
- Datos consolidados (02_consolidation/data/consolidated/)
- Logs de ejecuci√≥n por fuente

**Frecuencia:** Manual o programada (modo incremental o full)

**Dependencias:**
- BumeranScraper, ComputRabajoMultiSearch
- ZonaJobsScraperFinal, LinkedInScraper, IndeedScraper
- ConsolidadorMultiFuente, IncrementalTracker

**Notas:**
Script maestro que permite ejecutar una o todas las fuentes con `--source=X` o `--full`.

---

### 2. run_scheduler.py
**Ruta:** `D:\OEDE\Webscrapping\run_scheduler.py`

**Prop√≥sito:**
Scheduler automatizado que ejecuta scraping de Bumeran seg√∫n calendario (lunes y jueves 8 AM por defecto).

**Inputs:**
- database/config.py (SCHEDULER_CONFIG)
- Diccionario v3.2 (keywords)

**Outputs:**
- SQLite database actualizada (ofertas + m√©tricas)
- Logs mensuales (logs/scheduler_YYYYMM.log)

**Frecuencia:** Loop continuo (2x por semana configurado)

**Dependencias:**
- BumeranMultiSearch, DatabaseManager
- schedule library

**Notas:**
Ejecutar con `python run_scheduler.py`. Loop infinito con heartbeat logging.

---

## 2Ô∏è‚É£ SCRAPING - FUENTES PRINCIPALES

### 3. bumeran_scraper.py
**Ruta:** `D:\OEDE\Webscrapping\01_sources\bumeran\scrapers\bumeran_scraper.py`

**Prop√≥sito:**
Scraper principal de Bumeran usando API REST (~12K ofertas disponibles).

**Inputs:**
- API POST: https://www.bumeran.com.ar/api/avisos/searchV2
- Headers: x-site-id, x-pre-session-token

**Outputs:**
- CSV/JSON/Excel en 01_sources/bumeran/data/raw/
- 40+ campos por oferta (fechas normalizadas, descripci√≥n limpia)

**Frecuencia:** Llamado por pipeline o scheduler

**Dependencias:**
- IncrementalTracker (evita duplicados)
- BumeranSchemas (validaci√≥n)
- ScrapingMetrics, AdaptiveRateLimiter, CircuitBreaker, AlertManager

**Notas:**
Implementa retry autom√°tico, validaci√≥n de schema, detecci√≥n de paginaci√≥n rota. Fuente principal del proyecto.

---

### 4. scrapear_con_diccionario.py
**Ruta:** `D:\OEDE\Webscrapping\01_sources\bumeran\scrapers\scrapear_con_diccionario.py`

**Prop√≥sito:**
Scraper multi-keyword de Bumeran que itera diccionario de b√∫squedas para m√°xima cobertura.

**Inputs:**
- Diccionario maestro:
  - Estrategia m√°xima: ~90 keywords
  - Estrategia completa: ~55 keywords
  - Estrategia general: ~30 keywords
- data/config/keywords_loader.py

**Outputs:**
- DataFrame consolidado deduplicado
- M√©tricas de cobertura vs API directa

**Frecuencia:** Modo incremental (diaria) o full (primera ejecuci√≥n)

**Dependencias:**
- BumeranScraper, IncrementalTracker
- keywords_loader, date_filter

**Notas:**
Detecta duplicados masivos (paginaci√≥n rota), filtro temporal de 7 d√≠as por defecto.

---

### 5. computrabajo_scraper.py
**Ruta:** `D:\OEDE\Webscrapping\01_sources\computrabajo\scrapers\computrabajo_scraper.py`

**Prop√≥sito:**
Scraper de ComputRabajo usando HTML parsing (BeautifulSoup) - ~500-1K ofertas.

**Inputs:**
- URL base: https://ar.computrabajo.com
- Paginaci√≥n HTML

**Outputs:**
- CSV/JSON/Excel con 20+ campos estructurados

**Frecuencia:** Llamado por pipeline

**Dependencias:**
- BeautifulSoup4, requests

**Notas:**
Modo r√°pido (solo listados) o lento (fetch_description=True). Menor volumen que Bumeran.

---

### 6. linkedin_scraper.py
**Ruta:** `D:\OEDE\Webscrapping\01_sources\linkedin\scrapers\linkedin_scraper.py`

**Prop√≥sito:**
Scraper de LinkedIn usando JobSpy library con estrategia multi-keyword.

**Inputs:**
- JobSpy library (wrapper de LinkedIn API)
- Diccionario de keywords (30 keywords general)

**Outputs:**
- CSV/JSON/Excel consolidado deduplicado

**Frecuencia:** Llamado por pipeline

**Dependencias:**
- jobspy library
- IncrementalTracker

**Notas:**
L√≠mite conservador ~500 ofertas/keyword por rate limits de LinkedIn. Requiere validaci√≥n manual peri√≥dica.

---

### 7. zonajobs_scraper_final.py
**Ruta:** `D:\OEDE\Webscrapping\01_sources\zonajobs\scrapers\zonajobs_scraper_final.py`

**Prop√≥sito:**
Scraper de ZonaJobs usando API REST (~5K ofertas).

**Inputs:**
- POST: https://www.zonajobs.com.ar/api/avisos/searchHomeV2
- Headers similares a Bumeran

**Outputs:**
- JSON/CSV/Excel parseados y normalizados

**Frecuencia:** Llamado por pipeline

**Dependencias:**
- IncrementalTracker
- ZonaJobsParser (normalizaci√≥n espec√≠fica)

**Notas:**
Filtro local de keywords (API no soporta filtros complejos). Segunda fuente m√°s importante.

---

## 3Ô∏è‚É£ CONSOLIDACI√ìN Y NORMALIZACI√ìN

### 8. consolidar_fuentes.py
**Ruta:** `D:\OEDE\Webscrapping\02_consolidation\scripts\consolidar_fuentes.py`

**Prop√≥sito:**
Consolida y normaliza datos de m√∫ltiples fuentes a schema unificado.

**Inputs:**
- CSV/JSON de 01_sources/*/data/raw/
- Normalizadores por fuente (Bumeran, ZonaJobs, ComputRabajo, LinkedIn, Indeed)

**Outputs:**
- 02_consolidation/data/consolidated/*.csv|json|xlsx
- Reporte de cobertura por fuente

**Frecuencia:** Llamado por pipeline despu√©s de scraping

**Dependencias:**
- normalizar_campos.py (BumeranNormalizer, ZonaJobsNormalizer, etc.)

**Notas:**
Soporta filtrado por fechas. Output final: 40+ campos normalizados.

---

### 9. normalizar_campos.py
**Ruta:** `D:\OEDE\Webscrapping\02_consolidation\scripts\normalizar_campos.py`

**Prop√≥sito:**
Normalizadores que convierten datos crudos de cada fuente al schema unificado (40+ campos estructurados).

**Inputs:**
- DataFrames crudos por fuente (schemas heterog√©neos)

**Outputs:**
- DataFrames normalizados con estructura com√∫n:
  - titulo, descripcion, empresa, ubicacion
  - fecha_publicacion (ISO), modalidad_trabajo, tipo_jornada
  - salario_min/max, requisitos, etc.

**Frecuencia:** Llamado por consolidar_fuentes.py

**Dependencias:**
- BaseNormalizer (clase base abstracta)

**Notas:**
Incluye limpieza HTML, normalizaci√≥n de modalidad/tipo trabajo, conversi√≥n de fechas a ISO.

---

### 10. incremental_tracker.py
**Ruta:** `D:\OEDE\Webscrapping\02_consolidation\scripts\incremental_tracker.py`

**Prop√≥sito:**
Sistema de tracking de IDs scrapeados por fuente para modo incremental (evita duplicados entre ejecuciones).

**Inputs:**
- data/tracking/{source}_scraped_ids.json

**Outputs:**
- JSON con IDs + timestamps (formato v2.0 con operaciones at√≥micas)

**Frecuencia:** Usado en cada scraping

**Dependencias:**
- Ninguna (librer√≠a est√°ndar Python)

**Notas:**
Escritura at√≥mica con backups, detecci√≥n de formato legacy. Cr√≠tico para scraping incremental eficiente.

---

## 4Ô∏è‚É£ EXTRACCI√ìN NLP

### 11. run_nlp_extraction.py
**Ruta:** `D:\OEDE\Webscrapping\02.5_nlp_extraction\scripts\run_nlp_extraction.py`

**Prop√≥sito:**
Script principal para ejecutar extracci√≥n NLP en datasets completos (extrae requisitos, educaci√≥n, experiencia, salario).

**Inputs:**
- CSV de 01_sources/*/data/raw/
- Extractores por fuente (Bumeran, ZonaJobs, Indeed)

**Outputs:**
- 02.5_nlp_extraction/data/processed/*_nlp_YYYYMMDD.csv
- Stats de extracci√≥n:
  - Confidence score promedio
  - Cobertura por campo (% no nulos)
  - Distribuci√≥n de valores

**Frecuencia:** Manual despu√©s de consolidaci√≥n

**Dependencias:**
- BumeranExtractor, ZonaJobsExtractor, IndeedExtractor

**Notas:**
Procesa en batches para datasets grandes. Reporta m√©tricas de calidad.

---

### 12. bumeran_extractor.py
**Ruta:** `D:\OEDE\Webscrapping\02.5_nlp_extraction\scripts\extractors\bumeran_extractor.py`

**Prop√≥sito:**
Extractor NLP espec√≠fico para Bumeran usando regex patterns v3 + limpieza de texto.

**Inputs:**
- DataFrame con columnas: descripcion, titulo

**Outputs:**
- DataFrame con campos extra√≠dos:
  - experiencia_a√±os_min/max
  - nivel_educativo (primaria, secundaria, terciaria, universitaria, posgrado)
  - salario_min/max
  - idiomas (espa√±ol, ingl√©s, portugu√©s, etc.)
  - certificaciones
  - modalidad_trabajo (presencial, h√≠brido, remoto)

**Frecuencia:** Llamado por run_nlp_extraction.py

**Dependencias:**
- regex_patterns_v3 (patrones optimizados)
- text_cleaner, html_stripper, encoding_fixer

**Notas:**
Versi√≥n v3 optimizada (backups v1, v2 disponibles en carpeta). Mayor precisi√≥n vs v1/v2.

---

### 13. regex_patterns_v3.py
**Ruta:** `D:\OEDE\Webscrapping\02.5_nlp_extraction\scripts\patterns\regex_patterns_v3.py`

**Prop√≥sito:**
Patrones regex optimizados para extracci√≥n de informaci√≥n laboral (v3 m√°s preciso que v1/v2).

**Inputs:**
- Texto de ofertas (descripci√≥n)

**Outputs:**
- Matches de:
  - Experiencia: "2-3 a√±os", "m√≠nimo 5 a√±os", "junior/semi-senior/senior"
  - Educaci√≥n: "t√≠tulo universitario", "estudiante avanzado", "terciario completo"
  - Idiomas: "ingl√©s avanzado", "portugu√©s intermedio"
  - Certificaciones: nombres comunes de certificaciones
  - Modalidad: "trabajo remoto", "presencial", "modalidad h√≠brida"
  - Salario: "$50.000 a $80.000", "hasta $100k"

**Frecuencia:** Importado por extractores

**Dependencias:**
- re (Python regex)

**Notas:**
M√∫ltiples versiones mantenidas (v1, v2, v3) para A/B testing. v3 actual.

---

## 5Ô∏è‚É£ ESCO MATCHING

### 14. esco_hybrid_matcher.py
**Ruta:** `D:\OEDE\Webscrapping\03_esco_matching\scripts\esco_hybrid_matcher.py`

**Prop√≥sito:**
Matcher h√≠brido que combina Fuzzy (r√°pido score‚â•80) + Embeddings (similitud sem√°ntica) + LLM (Ollama para casos complejos score<70).

**Inputs:**
- T√≠tulos de ofertas de trabajo
- ESCO JSON (3,008 ocupaciones)
- Modelo embeddings: paraphrase-multilingual-MiniLM-L12-v2

**Outputs:**
- Mejor match ESCO:
  - esco_code (c√≥digo ISCO-08)
  - occupation_name
  - confidence_score (0-100)
  - strategy ("fuzzy" / "embeddings" / "llm")

**Frecuencia:** Manual para clasificaci√≥n de ofertas

**Dependencias:**
- sentence-transformers (embeddings)
- Ollama (llama3 para LLM fallback)
- difflib (fuzzy matching)

**Notas:**
Sistema de 3 niveles: Fuzzy primero (r√°pido), si falla ‚Üí Embeddings, si falla ‚Üí LLM (lento pero preciso).

---

### 15. integracion_esco_semantica.py
**Ruta:** `D:\OEDE\Webscrapping\03_esco_matching\scripts\integracion_esco_semantica.py`

**Prop√≥sito:**
Integraci√≥n final de ofertas NLP procesadas con taxonom√≠a ESCO + skills asociadas.

**Inputs:**
- Ofertas con NLP extra√≠do (CSV)
- ESCO occupations (3,008 ocupaciones)
- ESCO skills (13,890 skills)
- ISCO hierarchy (jerarqu√≠a de ocupaciones)

**Outputs:**
- Dataset final con:
  - Clasificaci√≥n ESCO (c√≥digo + nombre)
  - Skills asociadas a la ocupaci√≥n
  - Jerarqu√≠a ISCO (major group ‚Üí sub-major ‚Üí minor ‚Üí unit)
  - Confidence score

**Frecuencia:** Manual despu√©s de NLP

**Dependencias:**
- esco_hybrid_matcher.py

**Notas:**
Output final listo para dashboard Shiny. Enriquece ofertas con taxonom√≠a est√°ndar europea.

---

## 6Ô∏è‚É£ BASE DE DATOS

### 16. db_manager.py
**Ruta:** `D:\OEDE\Webscrapping\database\db_manager.py`

**Prop√≥sito:**
Gestor principal de SQLite con dual-write al schema v2 (ofertas, m√©tricas, alertas, circuit breaker, rate limiter).

**Inputs:**
- DataFrames de ofertas procesadas
- M√©tricas de ScrapingMetrics
- Alertas de AlertManager

**Outputs:**
- SQLite database: bumeran_scraping.db
- Tablas:
  - ofertas (datos principales)
  - metricas_scraping (performance)
  - alertas (errores/warnings)
  - circuit_breaker_stats (resiliencia)
  - rate_limiter_stats (control de velocidad)

**Frecuencia:** Usado por scheduler y pipeline

**Dependencias:**
- db_manager_v2 (dual-write para migraci√≥n)
- sqlite3

**Notas:**
Validaci√≥n de calidad pre-insert. Rechaza ofertas con campos cr√≠ticos vac√≠os (titulo, empresa).

---

### 17. init_sqlite.py
**Ruta:** `D:\OEDE\Webscrapping\database\init_sqlite.py`

**Prop√≥sito:**
Inicializador de database SQLite (crea schema con 5+ tablas + vistas).

**Inputs:**
- create_database_sqlite.sql (DDL statements)

**Outputs:**
- bumeran_scraping.db con:
  - Foreign keys habilitados
  - √çndices en campos clave
  - Vistas para reportes

**Frecuencia:** Una vez al inicio del proyecto (o con flag --reset)

**Dependencias:**
- sqlite3

**Notas:**
Incluye verificaci√≥n de tablas requeridas. Safe to re-run (no pierde datos).

---

### 18. populate_esco_from_rdf.py
**Ruta:** `D:\OEDE\Webscrapping\database\populate_esco_from_rdf.py`

**Prop√≥sito:**
Parser del archivo ESCO RDF (1.26 GB) que extrae 3,008 ocupaciones + 13,890 skills + jerarqu√≠a ISCO + 60K+ associations.

**Inputs:**
- esco-v1.2.0.rdf (solo espa√±ol xml:lang="es")
- Archivo RDF completo de taxonom√≠a ESCO

**Outputs:**
- Tablas SQLite:
  - esco_occupations (3,008 filas)
  - esco_skills (13,890 filas)
  - esco_isco (jerarqu√≠a ISCO-08)
  - esco_associations (60K+ relaciones ocupaci√≥n-skill)

**Frecuencia:** Una vez al inicio (15-30 min procesamiento)

**Dependencias:**
- rdflib (parser RDF/XML)
- tqdm (progress bar)

**Notas:**
Extracci√≥n completa de metadata ESCO (alternative labels, scope notes, ancestors ISCO). Solo ejecutar si la BD no tiene tablas ESCO.

---

## 7Ô∏è‚É£ DASHBOARDS Y AN√ÅLISIS

### 19. app.R
**Ruta:** `D:\OEDE\Webscrapping\Visual--\app.R`

**Prop√≥sito:**
Dashboard Shiny interactivo para an√°lisis del mercado laboral con taxonom√≠a ESCO (268 ofertas matcheadas actualmente).

**Inputs:**
- CSV con ofertas + clasificaci√≥n ESCO
- Datos de skills, provincias, ocupaciones

**Outputs:**
- Dashboard web con 6 tabs:
  1. **Panorama General:** M√©tricas clave, mapa de provincias
  2. **Perfil Demandado:** Educaci√≥n, experiencia, salarios
  3. **Skills M√°s Demandadas:** Top skills por ocupaci√≥n
  4. **Ocupaciones ESCO:** Taxonom√≠a y distribuci√≥n
  5. **Explorador de Ofertas:** B√∫squeda y filtros
  6. **Tendencias:** Series temporales
- Gr√°ficos interactivos: plotly, ggplot2
- Mapas: leaflet
- Tablas: DT

**Frecuencia:** On-demand (deployment en shinyapps.io)

**Dependencias:**
- shiny, shinydashboard
- ggplot2, plotly, leaflet, DT

**Notas:**
Versi√≥n v2.4.0 con filtro temporal, responsive design. shinyTree deshabilitado por conflictos de versi√≥n. Desplegado en: https://dos1tv-gerardo-breard.shinyapps.io/dashboard-esco-argentina/

---

### 20. dashboard_scraping_v4.py
**Ruta:** `D:\OEDE\Webscrapping\dashboard_scraping_v4.py`

**Prop√≥sito:**
Dashboard Python de monitoreo de scraping en tiempo real (m√©tricas, alertas, cobertura).

**Inputs:**
- SQLite database (tablas: metricas_scraping, alertas)
- Logs de scraping

**Outputs:**
- Dashboard web (Streamlit/Dash) con:
  - M√©tricas de performance (tiempo/oferta, tasa √©xito)
  - Visualizaciones de cobertura por fuente
  - Alertas y errores recientes
  - Circuit breaker status
  - Rate limiter stats

**Frecuencia:** On-demand para monitoreo de operaciones

**Dependencias:**
- streamlit o dash
- pandas, plotly

**Notas:**
Versi√≥n v4 actual (v1-v3 deprecated). Dashboard t√©cnico para DevOps, no p√∫blico.

---

## üìä FLUJO T√çPICO DE EJECUCI√ìN

### Pipeline Completo (modo manual):

```bash
# 1. Scraping multi-fuente
python run_full_pipeline.py --full

# 2. Consolidaci√≥n (incluida en pipeline)
# ‚Üí consolidar_fuentes.py se ejecuta autom√°ticamente

# 3. Extracci√≥n NLP (manual)
python 02.5_nlp_extraction/scripts/run_nlp_extraction.py

# 4. Clasificaci√≥n ESCO (manual)
python 03_esco_matching/scripts/integracion_esco_semantica.py

# 5. Visualizaci√≥n
# ‚Üí Iniciar dashboard Shiny
Rscript -e "shiny::runApp('Visual--/app.R', port=3840)"
```

### Pipeline Automatizado (modo scheduler):

```bash
# Ejecutar scheduler (loop continuo)
python run_scheduler.py

# ‚Üí Ejecuta autom√°ticamente 2x/semana:
#   - Scraping Bumeran con diccionario
#   - Guardado en SQLite
#   - Logs en logs/scheduler_YYYYMM.log
```

---

## üîÑ SCRIPTS CON VERSIONES M√öLTIPLES

### Usar √öLTIMA versi√≥n en producci√≥n:

1. **bumeran_extractor.py**
   - ‚úÖ v3 (actual): `02.5_nlp_extraction/scripts/extractors/bumeran_extractor.py`
   - üì¶ v2 (backup): `02.5_nlp_extraction/scripts/extractors/bumeran_extractor_v2.py`
   - üì¶ v1 (legacy): `02.5_nlp_extraction/scripts/extractors/bumeran_extractor_v1.py`

2. **regex_patterns.py**
   - ‚úÖ v3 (actual): `02.5_nlp_extraction/scripts/patterns/regex_patterns_v3.py`
   - üì¶ v2 (backup): `02.5_nlp_extraction/scripts/patterns/regex_patterns_v2.py`
   - üì¶ v1 (legacy): `02.5_nlp_extraction/scripts/patterns/regex_patterns_v1.py`

3. **dashboard_scraping.py**
   - ‚úÖ v4 (actual): `dashboard_scraping_v4.py`
   - üì¶ v3 (deprecated): `dashboard_scraping_v3.py`
   - üì¶ v2 (deprecated): `dashboard_scraping_v2.py`
   - üì¶ v1 (deprecated): `dashboard_scraping.py`

4. **app.R**
   - ‚úÖ v2.4.0 (actual): `Visual--/app.R`
   - üì¶ v3 (backup): `Visual--/backup/app_v3.R`
   - üì¶ v2.0 (backup): `Visual--/backup/app_v2.0_pre-mejoras.R`

---

## üìå FRECUENCIAS DE EJECUCI√ìN

### Autom√°tica (sin intervenci√≥n):
- **2x por semana:** run_scheduler.py (lunes y jueves 8 AM)

### Manual (cuando hay nuevos datos):
- **Incremental diario:** run_full_pipeline.py --source=bumeran
- **Full semanal:** run_full_pipeline.py --full (todas las fuentes)

### Post-procesamiento (despu√©s de scraping):
- **NLP:** run_nlp_extraction.py (procesar ofertas nuevas)
- **ESCO:** integracion_esco_semantica.py (clasificar nuevas ofertas)

### Setup √∫nico (primera vez):
- **Database:** init_sqlite.py --reset
- **ESCO:** populate_esco_from_rdf.py

### On-demand (cuando se necesita):
- **Dashboard Shiny:** Rscript -e "shiny::runApp('Visual--/app.R')"
- **Dashboard Python:** python dashboard_scraping_v4.py

---

## üéØ SCRIPTS EXCLUIDOS DEL TOP 20

Los siguientes tipos de scripts NO fueron incluidos (razones):

1. **Tests (43 scripts):** test_*.py - √ötiles para desarrollo pero no cr√≠ticos para producci√≥n
2. **An√°lisis exploratorios:** analizar_*.py, explorar_*.py - Temporales/ad-hoc
3. **Backups versionados:** *_v1.py, *_v2.py, *_backup.py - Versiones obsoletas
4. **Utils gen√©ricos:** check_*.py, verify_*.py, fix_*.py - Mantenimiento secundario
5. **Deployment R:** instalar_*.R, deploy_*.R, configurar_*.R - Setup √∫nico
6. **Migraciones:** migrate_*.py - Ejecuci√≥n √∫nica hist√≥rica
7. **Scripts duplicados:** Webscrapping/ folder (duplicados de Visual--/)

---

## üìû CONTACTO

**Responsable:** Equipo T√©cnico OEDE
**√öltima actualizaci√≥n:** 14/11/2025
**Versi√≥n del documento:** 1.0

---

**Documentos relacionados:**
- `inventario_scripts_python.txt` - Listado completo de 213 scripts Python
- `inventario_scripts_R.txt` - Listado completo de 35 scripts R
- `schema_bd.sql` - Schema de 32 tablas SQLite
- `ARQUITECTURA_SISTEMA.md` - Diagrama de arquitectura (pr√≥ximo)
