# Arquitectura del Sistema

**VersiÃ³n**: 4.0
**Fecha**: 2025-10-21

---

## ğŸ¯ VisiÃ³n General

Sistema modular de web scraping multi-fuente con pipeline automatizado de 5 etapas que extrae, normaliza, clasifica y analiza ofertas laborales.

### Principios de DiseÃ±o

1. **SeparaciÃ³n de Responsabilidades**: Cada etapa es independiente
2. **Escalabilidad**: FÃ¡cil agregar nuevas fuentes
3. **Modularidad**: Componentes reutilizables
4. **Schema Ãšnico**: Formato comÃºn para todas las fuentes
5. **AutomatizaciÃ³n**: Pipeline end-to-end
6. **Trazabilidad**: Logs y metadatos en cada etapa

---

## ğŸ“ Arquitectura de 5 Etapas

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                        PIPELINE COMPLETO                     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â”‚
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚                     â”‚                     â”‚
        â–¼                     â–¼                     â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 01_sources    â”‚     â”‚ 02_consolida  â”‚    â”‚ 03_esco       â”‚
â”‚ - zonajobs    â”‚â”€â”€â”€â”€â–¶â”‚ - Normaliza   â”‚â”€â”€â”€â–¶â”‚ - Clasifica   â”‚
â”‚ - bumeran     â”‚     â”‚ - Deduplica   â”‚    â”‚ - Enriquece   â”‚
â”‚ - computra... â”‚     â”‚ - Valida      â”‚    â”‚               â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â”‚
                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                    â”‚                   â”‚
                    â–¼                   â–¼
            â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
            â”‚ 04_analysis   â”‚   â”‚ 05_products   â”‚
            â”‚ - EstadÃ­sticasâ”‚   â”‚ - Datasets    â”‚
            â”‚ - Visualiza.  â”‚   â”‚ - Reportes    â”‚
            â”‚ - Reportes    â”‚   â”‚ - APIs        â”‚
            â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ”§ Etapa 01: Sources (Fuentes)

### Responsabilidad
Extraer datos crudos de cada sitio web.

### CaracterÃ­sticas
- âœ… Scrapers independientes por fuente
- âœ… Rate limiting y respeto a robots.txt
- âœ… Datos crudos sin procesar
- âœ… Logs por fuente
- âœ… ConfiguraciÃ³n independiente

### Estructura
```
01_sources/
â”œâ”€â”€ zonajobs/
â”‚   â”œâ”€â”€ scrapers/           # CÃ³digo de scraping
â”‚   â”œâ”€â”€ data/raw/          # Datos crudos
â”‚   â”œâ”€â”€ config/            # Config especÃ­fica
â”‚   â””â”€â”€ README.md
â””â”€â”€ [otras_fuentes]/
```

### TecnologÃ­as
- `requests` / `httpx` para HTTP
- `BeautifulSoup` / `lxml` para parsing HTML
- `Playwright` para JavaScript
- `pandas` para estructurar datos

### Output
```
zonajobs_raw_20251021_143000.csv
- Campos en formato original
- Sin normalizaciÃ³n
- Con metadatos de extracciÃ³n
```

---

## ğŸ”— Etapa 02: Consolidation (ConsolidaciÃ³n)

### Responsabilidad
Unificar datos de mÃºltiples fuentes en schema comÃºn.

### Proceso

```python
# 1. Leer datos crudos de cada fuente
for fuente in fuentes:
    df_raw = leer_datos_crudos(fuente)

    # 2. Normalizar al schema unificado
    normalizer = get_normalizer(fuente)
    df_norm = normalizer.normalize(df_raw)

    dfs.append(df_norm)

# 3. Consolidar todas
df_consolidated = pd.concat(dfs)

# 4. Deduplicar
df_unique = deduplicador.deduplicar(df_consolidated)

# 5. Validar
validar_schema(df_unique)
```

### Componentes Clave

#### Normalizadores
```python
class ZonaJobsNormalizer(BaseNormalizer):
    def normalize(self, df):
        # Mapea campos ZonaJobs â†’ Schema Unificado
        return df_normalized
```

#### Deduplicador
```python
class DeduplicadorOfertas:
    def deduplicar(self, df):
        # 1. Por ID exacto
        # 2. Por similitud de tÃ­tulo + empresa
        return df_unique
```

#### Validador
```python
class ValidadorSchema:
    def validar(self, df):
        # Verifica contra schema_unificado.json
        return resultados
```

### Output
```
ofertas_consolidadas_20251021.csv
- Schema unificado
- Sin duplicados
- Validado
```

---

## ğŸ¯ Etapa 03: ESCO Matching

### Responsabilidad
Clasificar ofertas con taxonomÃ­a ESCO y enriquecer con skills.

### Proceso

```python
# 1. Cargar datos consolidados
df = pd.read_csv('ofertas_consolidadas.csv')

# 2. Matching semÃ¡ntico
matcher = TFIDFMatcher(threshold=0.7)
for idx, row in df.iterrows():
    titulo = row['informacion_basica.titulo']

    # Match con ESCO
    match = matcher.match(titulo)

    df.loc[idx, 'clasificacion_esco.ocupacion'] = match['label']
    df.loc[idx, 'clasificacion_esco.isco_code'] = match['isco']
    df.loc[idx, 'clasificacion_esco.score'] = match['score']

# 3. Enriquecimiento con skills
df = enriquecer_skills(df)
```

### Algoritmos Disponibles

1. **TF-IDF + Cosine Similarity**
   - RÃ¡pido
   - PrecisiÃ³n: ~70%

2. **Embeddings SemÃ¡nticos**
   - MÃ¡s lento
   - PrecisiÃ³n: ~85%
   - Modelo: `paraphrase-multilingual-mpnet-base-v2`

3. **HÃ­brido**
   - Combina ambos
   - PrecisiÃ³n: ~80%

### Enriquecimiento

```python
# Para cada oferta clasificada
for ocupacion_code in df['clasificacion_esco.ocupacion_esco_code']:
    # Buscar skills asociadas en ESCO
    skills = esco_skills[ocupacion_code]

    # Filtrar por tipo
    essential_skills = [s for s in skills if s['type'] == 'essential']
    optional_skills = [s for s in skills if s['type'] == 'optional']
```

### Output
```
ofertas_esco_matched_20251021.csv
- Todas las ofertas con cÃ³digo ISCO
- Skills asociadas
- Scores de similitud
```

---

## ğŸ“Š Etapa 04: Analysis (AnÃ¡lisis)

### Responsabilidad
Generar estadÃ­sticas, visualizaciones y reportes.

### AnÃ¡lisis Implementados

#### 1. AnÃ¡lisis Descriptivo
```python
- Total de ofertas
- DistribuciÃ³n por fuente
- Top provincias
- Top ocupaciones ISCO
- Modalidades de trabajo
```

#### 2. AnÃ¡lisis Temporal
```python
- Series temporales diarias/semanales
- Tendencias por grupo ISCO
- Heatmaps dÃ­a Ã— mes
- Ocupaciones emergentes
```

#### 3. AnÃ¡lisis de Skills
```python
- Top 50 skills mÃ¡s demandadas
- Skills por ocupaciÃ³n
- Matriz de co-ocurrencia
```

#### 4. AnÃ¡lisis GeogrÃ¡fico
```python
- DistribuciÃ³n por provincia
- Ofertas remotas vs presenciales
- Densidad geogrÃ¡fica
```

### Visualizaciones

```python
# Generadas automÃ¡ticamente
1. distribucion_isco.png           # Barras top 20 ISCO
2. temporal_ofertas.png            # Serie temporal
3. heatmap_dia_mes.png             # Heatmap
4. modalidades_trabajo.png         # Pie chart
5. top_skills.png                  # Wordcloud
6. salarios_isco.png               # Boxplots
...
13. dashboard_interactivo.html     # Plotly dashboard
```

### Reportes

```python
# Formatos disponibles
- HTML: Reporte interactivo auto-contenido
- PDF: Reporte imprimible
- Excel: Datos + grÃ¡ficos
- PowerPoint: PresentaciÃ³n ejecutiva
```

### Output
```
04_analysis/outputs/
â”œâ”€â”€ reports/
â”‚   â”œâ”€â”€ reporte_completo.html
â”‚   â””â”€â”€ reporte_ejecutivo.pdf
â””â”€â”€ figures/
    â”œâ”€â”€ distribucion_isco.png
    â””â”€â”€ ...
```

---

## ğŸ“¦ Etapa 05: Products (Productos)

### Responsabilidad
Empaquetar datasets y reportes para distribuciÃ³n.

### Productos

#### 1. Datasets Publicables
```
ofertas_laborales_argentina_2025.csv
ofertas_isco_clasificadas_2025.xlsx
diccionario_datos.md
metadata.json
```

#### 2. Reportes Finales
```
informe_anual_mercado_laboral_2025.pdf
dashboard_ofertas_2025.html
```

#### 3. API REST (opcional)
```python
FastAPI
â”œâ”€â”€ GET /ofertas
â”œâ”€â”€ GET /ofertas/{id}
â”œâ”€â”€ GET /estadisticas/isco
â””â”€â”€ GET /skills/top
```

### Metadatos

```json
{
  "dataset": "Ofertas Laborales Argentina",
  "version": "2025-10-21",
  "fuentes": ["zonajobs"],
  "total_ofertas": 61,
  "fecha_desde": "2025-10-01",
  "fecha_hasta": "2025-10-21",
  "cobertura_esco": "73.8%",
  "licencia": "CC BY 4.0"
}
```

---

## ğŸ”„ Pipeline Automatizado

### OrquestaciÃ³n

```python
class PipelineCompleto:
    def ejecutar_completo(self):
        # Etapa 1: Scraping
        self.etapa_01_scraping()

        # Etapa 2: ConsolidaciÃ³n
        self.etapa_02_consolidacion()

        # Etapa 3: ESCO Matching
        self.etapa_03_esco_matching()

        # Etapa 4: AnÃ¡lisis
        self.etapa_04_analisis()

        # Etapa 5: Productos
        self.etapa_05_productos()
```

### EjecuciÃ³n

```bash
# Todo el pipeline
python pipeline_completo.py --all

# Desde una etapa especÃ­fica
python pipeline_completo.py --desde-consolidacion

# Solo algunas etapas
python pipeline_completo.py --scraping --consolidacion
```

---

## ğŸ—„ï¸ Schema Unificado

### DiseÃ±o

```json
{
  "_metadata": {...},           // Origen y extracciÃ³n
  "informacion_basica": {...},  // TÃ­tulo, empresa, desc
  "ubicacion": {...},           // Provincia, ciudad
  "modalidad": {...},           // Tipo y modalidad trabajo
  "fechas": {...},              // PublicaciÃ³n, cierre
  "requisitos": {...},          // Experiencia, educaciÃ³n
  "compensacion": {...},        // Salario, beneficios
  "detalles": {...},            // Vacantes, Ã¡rea
  "clasificacion_esco": {...},  // ESCO + ISCO
  "source_specific": {...}      // Campos especÃ­ficos fuente
}
```

### Ventajas

1. **Interoperabilidad**: Mismo formato para todas las fuentes
2. **Extensibilidad**: `source_specific` para campos Ãºnicos
3. **Trazabilidad**: `_metadata` con origen
4. **ValidaciÃ³n**: JSON Schema para verificar
5. **Versionado**: Campo `version` en metadata

---

## ğŸ“ˆ Escalabilidad

### Agregar Nueva Fuente

```bash
# 1. Crear estructura
mkdir -p 01_sources/nueva_fuente/{scrapers,data/raw,config}

# 2. Implementar scraper
# 3. Crear normalizador
# 4. Listo - el pipeline lo detecta automÃ¡ticamente
```

### Horizontal Scaling

```python
# Paralelizar scraping
from concurrent.futures import ThreadPoolExecutor

with ThreadPoolExecutor(max_workers=4) as executor:
    futures = [
        executor.submit(scrapear, fuente)
        for fuente in fuentes
    ]
```

### Vertical Scaling

```python
# Usar bases de datos en lugar de CSVs
# PostgreSQL con SQLAlchemy
# Particionamiento por fecha
```

---

## ğŸ” Seguridad y Buenas PrÃ¡cticas

### Scraping Ã‰tico

```python
1. Rate limiting (mÃ­n 2 seg)
2. Respetar robots.txt
3. User-Agent identificable
4. Solo datos pÃºblicos
5. Uso acadÃ©mico/investigaciÃ³n
```

### Datos Sensibles

```python
# No guardar:
- Emails personales
- TelÃ©fonos directos
- Datos de solicitantes

# Anonimizar:
- Empresas si son confidenciales
```

### ValidaciÃ³n

```python
# Antes de publicar
1. Validar schema
2. Verificar calidad
3. Remover duplicados
4. Anonimizar si necesario
5. Documentar metadatos
```

---

## ğŸ“Š Monitoreo

### Logs Centralizados

```
shared/logs/
â”œâ”€â”€ pipeline.log              # Log general
â”œâ”€â”€ consolidacion.log         # Etapa 2
â”œâ”€â”€ esco_matching.log         # Etapa 3
â””â”€â”€ analysis.log              # Etapa 4
```

### MÃ©tricas

```python
- Ofertas/hora scrapeadas
- % cobertura ESCO
- Tiempo por etapa
- Errores por fuente
- Calidad de matching
```

---

## ğŸ§ª Testing

### Tests Unitarios
```python
tests/unit/
â”œâ”€â”€ test_normalizers.py
â”œâ”€â”€ test_esco_matcher.py
â””â”€â”€ test_deduplicacion.py
```

### Tests de IntegraciÃ³n
```python
tests/integration/
â””â”€â”€ test_pipeline_completo.py
```

---

## ğŸ“š TecnologÃ­as Utilizadas

| Componente | TecnologÃ­a |
|---|---|
| Scraping | requests, Playwright, BeautifulSoup |
| Procesamiento | pandas, numpy |
| NLP/Matching | scikit-learn, sentence-transformers |
| VisualizaciÃ³n | matplotlib, seaborn, plotly |
| ValidaciÃ³n | jsonschema |
| Pipeline | Python subprocess |
| Testing | pytest |

---

**Arquitectura diseÃ±ada para escalar y evolucionar** ğŸš€
