# MOL - Issues en Formato Est√°ndar

> **Proyecto:** https://linear.app/molar/project/mol-monitor-ofertas-laborales-2a9662bfa15f
> **√öltima actualizaci√≥n:** 2025-12-03
> **Formato:** Basado en plantilla CCT-RAG

---

## √çndice por Prioridad

### üî¥ Alta Prioridad
- [MOL-5](#mol-5-v84-resolver-errores-sector_funcion) - Resolver sector_funcion
- [MOL-6](#mol-6-expandir-gold-set-a-50-casos) - Expandir Gold Set
- [MOL-18](#mol-18-automatizar-scrapers-faltantes) - Automatizar scrapers
- [MOL-26](#mol-26-backup-autom√°tico-de-sqlite) - Backup autom√°tico (NUEVO)
- [MOL-23](#mol-23-versionado-de-datos) - Versionado de datos (NUEVO)

### üü° Media Prioridad
- [MOL-7](#mol-7-m√©tricas-recall-y-f1) - M√©tricas Recall/F1
- [MOL-8](#mol-8-resolver-casos-biling√ºes) - Casos biling√ºes
- [MOL-19](#mol-19-pipeline-autom√°tico-post-scraping) - Pipeline autom√°tico
- [MOL-14](#mol-14-alertas-emailslack) - Alertas
- [MOL-16](#mol-16-fix-shinytree) - Fix shinyTree
- [MOL-24](#mol-24-entity-resolution-cross-source) - Entity resolution (NUEVO)
- [MOL-25](#mol-25-drift-detection) - Drift detection (NUEVO)

### ‚ö™ Baja Prioridad
- [MOL-9](#mol-9-cicd-github-actions) - CI/CD
- [MOL-10](#mol-10-regex-abreviaciones-argentinas) - Regex abreviaciones
- [MOL-11](#mol-11-niveles-jer√°rquicos) - Niveles jer√°rquicos
- [MOL-12](#mol-12-consolidar-nlp-v6v7) - Consolidar NLP
- [MOL-13](#mol-13-panel-administraci√≥n) - Panel admin
- [MOL-15](#mol-15-limpieza-jsons) - Limpieza JSONs
- [MOL-17](#mol-17-auth-shinymanager) - Auth shinymanager
- [MOL-20](#mol-20-centralizar-logs) - Centralizar logs
- [MOL-21](#mol-21-deprecar-dashboards) - Deprecar dashboards
- [MOL-22](#mol-22-documentar-apis) - Documentar APIs

---

# üî¥ ALTA PRIORIDAD

---

## MOL-5: [v8.4] Resolver errores sector_funcion

### Contexto
El 50% de los errores del gold set (4 de 8) son del tipo `sector_funcion`, donde el sistema mapea ofertas a ocupaciones ESCO de sectores completamente diferentes. Este es el error m√°s cr√≠tico porque produce clasificaciones absurdas (ej: "Ejecutivo de cuentas" ‚Üí "Agente de empleo").

**Historia de intentos:**
- v8.1: Ajustes por nivel jer√°rquico ‚Üí No resolvi√≥ sector_funcion
- v8.2: 6 familias funcionales ‚Üí Mejor√≥ categorizaci√≥n, no precisi√≥n
- v8.3: +4 familias espec√≠ficas ‚Üí Mejora parcial (57.9% ‚Üí 63.2%)

Referencia: `docs/CHANGELOG.md#v83`, `docs/TICKETS_CONTEXT.md#mol-5`

### Objetivo
Reducir errores de tipo `sector_funcion` de 4 casos a ‚â§1, alcanzando precisi√≥n ‚â•85% en gold set.

### Archivos involucrados
- `database/matching_rules_v84.py` - NUEVO (copiar de v83)
- `database/matching_rules_v83.py` - Referencia (NO modificar)
- `database/match_ofertas_multicriteria.py` - Referencia (NO modificar)
- `database/gold_set_manual_v1.json` - Referencia (casos de prueba)
- `database/test_gold_set_manual.py` - Modificar (apuntar a v84)
- `docs/CHANGELOG.md` - Agregar entrada v8.4

### Criterios de Aceptaci√≥n
- [ ] Precisi√≥n gold set ‚â•85% (actual: ~80%)
- [ ] Casos sector_funcion ‚â§1 (actual: 4)
- [ ] Caso "Ejecutivo de cuentas" (1118027276) matchea a ventas, no contadores
- [ ] Caso "Account Executive Hunter" (1118028887) matchea a comercial, no RRHH
- [ ] Caso "Analista administrativo" (1118028376) matchea a admin, no negocios
- [ ] Caso "Asesor comercial plan ahorro" (1118028833) matchea a ventas
- [ ] NO hay regresiones en casos que antes funcionaban
- [ ] Test pasa: `python database/test_gold_set_manual.py`
- [ ] Entrada agregada en `docs/CHANGELOG.md`

### Subtareas
- [ ] Analizar los 4 casos espec√≠ficos de sector_funcion (~1h)
  - Revisar embeddings actuales
  - Identificar por qu√© matchean incorrectamente
- [ ] Dise√±ar reglas para v8.4 (~2h)
  - Keywords VENTAS_B2B: "account executive", "hunter", "sales"
  - Keywords RRHH_ESCO: "agente de empleo", "reclutador"
  - Mapeo directo de t√≠tulos problem√°ticos
- [ ] Implementar `matching_rules_v84.py` (~2h)
  - Copiar v83 como base
  - Agregar nuevas familias/detectores
  - Agregar reglas never_confirm
- [ ] Validar con gold set (~30min)
  - Ejecutar test_gold_set_manual.py
  - Verificar m√©tricas antes/despu√©s
- [ ] Validar sin regresiones (~1h)
  - Batch piloto de 100 ofertas
  - Revisar manualmente 10 casos aleatorios
- [ ] Documentar en CHANGELOG.md (~30min)
- [ ] Actualizar Linear con resultados (~15min)

### Notas t√©cnicas
- El problema ra√≠z es que embeddings de t√≠tulos en ingl√©s ("Account Executive") est√°n m√°s cerca de ocupaciones de RRHH que de ventas en el espacio vectorial
- Posible approach: mapeo directo de t√≠tulos problem√°ticos antes del matching sem√°ntico
- Alternativa: boosting de score cuando oferta tiene keywords comerciales y ESCO tiene keywords ventas
- Priorizar precisi√≥n sobre recall: mejor no confirmar que confirmar mal
- Usar `never_confirm=True` para casos dudosos

### Referencias
- `docs/ARCHITECTURE.md#34-matching-esco`
- `docs/CHANGELOG.md#v83`
- Gold set casos: 1118027276, 1118028376, 1118028833, 1118028887

### Verificaci√≥n final
```bash
# Antes de cerrar el issue:
python database/test_gold_set_manual.py

# Output esperado:
# PRECISION: >= 85.0%
# ERRORES sector_funcion: <= 1
```

---

## MOL-6: Expandir Gold Set a 50+ casos

### Contexto
Con solo 19 casos, el gold set actual no es estad√≠sticamente representativo. Un cambio de 1 caso = 5.3pp de precisi√≥n, generando ruido en las m√©tricas. Adem√°s, hay sectores sin cobertura (IT: 0 casos) que impiden detectar problemas en esas √°reas.

Referencia: `docs/TICKETS_CONTEXT.md#mol-6`

### Objetivo
Expandir el gold set de 19 a 50+ casos con estratificaci√≥n por sector, nivel jer√°rquico y tipo de contrato.

### Archivos involucrados
- `database/gold_set_manual_v2.json` - NUEVO archivo
- `database/gold_set_manual_v1.json` - Referencia (mantener como backup)
- `database/test_gold_set_manual.py` - Modificar (cargar v2)
- `database/extract_stratified_sample.py` - Usar para muestreo
- `docs/CHANGELOG.md` - Agregar entrada

### Criterios de Aceptaci√≥n
- [ ] Gold set v2 tiene ‚â•50 casos
- [ ] Cobertura de sectores:
  - [ ] IT/Tech: ‚â•8 casos
  - [ ] Comercial/Ventas: ‚â•8 casos
  - [ ] Administrativo: ‚â•6 casos
  - [ ] Operarios/Producci√≥n: ‚â•6 casos
  - [ ] Salud: ‚â•5 casos
  - [ ] Servicios/Atenci√≥n: ‚â•5 casos
  - [ ] Legal: ‚â•4 casos
  - [ ] Marketing: ‚â•4 casos
- [ ] Cobertura de niveles:
  - [ ] Junior/Trainee: ‚â•10 casos
  - [ ] Semi-senior: ‚â•15 casos
  - [ ] Senior/Gerencial: ‚â•10 casos
- [ ] Cada caso tiene:
  - [ ] `id_oferta`
  - [ ] `esco_ok` (boolean)
  - [ ] `tipo_error` (si esco_ok=false)
  - [ ] `esco_esperado_uri` (para calcular recall)
  - [ ] `esco_esperado_label`
  - [ ] `sector` (para estratificaci√≥n)
  - [ ] `nivel` (para estratificaci√≥n)
  - [ ] `comentario`
- [ ] Test funciona con v2: `python database/test_gold_set_manual.py`
- [ ] Documentado en CHANGELOG.md

### Subtareas
- [ ] Generar muestra estratificada (~1h)
  ```bash
  python database/extract_stratified_sample.py --output muestreo_v2.csv --n 80
  ```
- [ ] Configurar dashboard de validaci√≥n (~30min)
  ```bash
  Rscript -e "shiny::runApp('Visual--/validacion_pipeline_app_v3.R', port=3853)"
  ```
- [ ] Validar casos IT/Tech: 10 casos (~1.5h)
- [ ] Validar casos Comercial/Ventas: 10 casos (~1.5h)
- [ ] Validar casos Administrativo: 8 casos (~1h)
- [ ] Validar casos Operarios: 8 casos (~1h)
- [ ] Validar casos Salud: 6 casos (~45min)
- [ ] Validar casos Servicios: 6 casos (~45min)
- [ ] Validar casos Legal: 5 casos (~30min)
- [ ] Validar casos Marketing: 5 casos (~30min)
- [ ] Compilar gold_set_manual_v2.json (~1h)
- [ ] Actualizar test_gold_set_manual.py (~30min)
- [ ] Documentar en CHANGELOG.md (~30min)

### Notas t√©cnicas
- Priorizar casos "dif√≠ciles" (scores entre 0.50-0.70) sobre casos obvios
- Incluir casos biling√ºes (t√≠tulos en ingl√©s)
- Incluir casos de pasant√≠as/trainee (programa vs ocupaci√≥n)
- Para `esco_esperado_uri`: buscar en https://esco.ec.europa.eu/
- Formato JSON:
```json
{
  "id_oferta": "1118027276",
  "esco_ok": false,
  "tipo_error": "sector_funcion",
  "esco_actual_uri": "http://data.europa.eu/esco/occupation/abc",
  "esco_esperado_uri": "http://data.europa.eu/esco/occupation/xyz",
  "esco_esperado_label": "Representante comercial",
  "sector": "comercial",
  "nivel": "semi-senior",
  "comentario": "T√≠tulo en ingl√©s confunde al modelo"
}
```

### Referencias
- `docs/ARCHITECTURE.md#7-testing-y-validaci√≥n`
- `Visual--/validacion_pipeline_app_v3.R`
- ESCO Portal: https://esco.ec.europa.eu/

### Verificaci√≥n final
```bash
# Verificar estructura del gold set:
python -c "
import json
with open('database/gold_set_manual_v2.json') as f:
    gs = json.load(f)
print(f'Total casos: {len(gs)}')
print(f'Campos requeridos: {list(gs[0].keys())}')
sectores = {}
for c in gs:
    s = c.get('sector', 'unknown')
    sectores[s] = sectores.get(s, 0) + 1
print(f'Por sector: {sectores}')
"
# Output esperado: Total casos: >= 50
```

---

## MOL-18: Automatizar scrapers faltantes

### Contexto
Solo Bumeran tiene scheduler automatizado (L/J 8am). Las otras 4 fuentes (ZonaJobs, Indeed, Computrabajo, LinkedIn) requieren ejecuci√≥n manual, lo que significa que el sistema solo captura ~20% del mercado de forma consistente.

Referencia: `docs/ARCHITECTURE.md#21-scraping`

### Objetivo
Automatizar la ejecuci√≥n de los 5 scrapers con manejo de errores, rate limiting y logging.

### Archivos involucrados
- `run_scheduler.py` - Modificar (agregar 4 fuentes)
- `01_sources/zonajobs/scrapers/zonajobs_scraper_final.py` - Verificar funcionamiento
- `01_sources/indeed/scrapers/indeed_scraper.py` - Verificar funcionamiento
- `01_sources/computrabajo/scrapers/computrabajo_scraper.py` - Verificar funcionamiento
- `01_sources/linkedin/scrapers/linkedin_scraper.py` - Verificar funcionamiento
- `config/scraper_config.yaml` - NUEVO (configuraci√≥n centralizada)
- `logs/scraper_*.log` - Output

### Criterios de Aceptaci√≥n
- [ ] Los 5 scrapers ejecutan autom√°ticamente
- [ ] Cada scraper tiene:
  - [ ] Rate limiting configurado
  - [ ] Reintentos con backoff exponencial
  - [ ] Timeout m√°ximo (30 min por fuente)
  - [ ] Logging a archivo dedicado
- [ ] Configuraci√≥n en archivo YAML (no hardcodeada)
- [ ] Scheduler funciona en Windows (Task Scheduler) o cron
- [ ] Manejo de errores no detiene otros scrapers
- [ ] Resumen al final con ofertas capturadas por fuente
- [ ] Test: ejecuci√≥n completa sin errores

### Subtareas
- [ ] Auditar estado de cada scraper (~2h)
  - [ ] ZonaJobs: verificar bypass Cloudflare funciona
  - [ ] Indeed: verificar rate limiting actual
  - [ ] Computrabajo: verificar HTML parsing
  - [ ] LinkedIn: verificar restricciones
- [ ] Crear config/scraper_config.yaml (~1h)
  ```yaml
  scrapers:
    bumeran:
      enabled: true
      rate_limit_seconds: 1.5
      timeout_minutes: 30
      retry_max: 3
    zonajobs:
      enabled: true
      rate_limit_seconds: 2.0
      # ...
  ```
- [ ] Modificar run_scheduler.py (~3h)
  - Cargar configuraci√≥n desde YAML
  - Loop por scrapers habilitados
  - Try/except por scraper (no detener si uno falla)
  - Logging estructurado
- [ ] Implementar logging por fuente (~1h)
  - `logs/scraper_bumeran_2025-12-03.log`
  - `logs/scraper_zonajobs_2025-12-03.log`
- [ ] Crear resumen de ejecuci√≥n (~1h)
  ```
  === RESUMEN SCRAPING 2025-12-03 ===
  Bumeran:     234 ofertas (OK)
  ZonaJobs:    156 ofertas (OK)
  Indeed:      ERROR - timeout
  Computrabajo: 89 ofertas (OK)
  LinkedIn:    SKIPPED - disabled
  Total:       479 ofertas
  ```
- [ ] Configurar Task Scheduler / cron (~1h)
- [ ] Test completo (~1h)
- [ ] Documentar en CHANGELOG.md (~30min)

### Notas t√©cnicas
- ZonaJobs requiere Playwright para bypass Cloudflare
- Indeed tiene rate limiting agresivo: usar delays de 3-5s
- LinkedIn tiene restricciones legales: considerar deshabilitar o usar solo API oficial
- Usar `tenacity` para reintentos con backoff
- Logs deben rotar (m√°ximo 30 d√≠as)

### Referencias
- `docs/ARCHITECTURE.md#21-scraping`
- `01_sources/bumeran/scrapers/bumeran_scraper.py` (ejemplo funcional)

### Verificaci√≥n final
```bash
# Test de ejecuci√≥n completa:
python run_scheduler.py --test-mode --limit 10

# Verificar logs generados:
ls -la logs/scraper_*.log

# Verificar ofertas capturadas:
sqlite3 database/bumeran_scraping.db "SELECT source, COUNT(*) FROM ofertas WHERE fecha_scraping = date('now') GROUP BY source"
```

---

## MOL-26: Backup autom√°tico de SQLite

### Contexto
Toda la data del proyecto est√° en un √∫nico archivo (`bumeran_scraping.db` de ~14MB). Un `rm` accidental, corrupci√≥n de disco o error de script podr√≠a eliminar meses de trabajo. Actualmente no hay backups automatizados.

### Objetivo
Implementar backup autom√°tico diario de la base de datos con retenci√≥n de 30 d√≠as.

### Archivos involucrados
- `scripts/backup_database.py` - NUEVO archivo
- `backups/` - NUEVO directorio
- `run_scheduler.py` - Modificar (agregar paso de backup)
- `.gitignore` - Agregar exclusi√≥n de backups

### Criterios de Aceptaci√≥n
- [ ] Backup diario autom√°tico despu√©s del scraping
- [ ] Formato: `backups/bumeran_scraping_YYYY-MM-DD.db.gz`
- [ ] Compresi√≥n gzip (~3MB comprimido)
- [ ] Retenci√≥n: √∫ltimos 30 backups
- [ ] Limpieza autom√°tica de backups antiguos
- [ ] Verificaci√≥n de integridad post-backup
- [ ] Log de backups realizados
- [ ] Script de restore documentado

### Subtareas
- [ ] Crear directorio `backups/` (~5min)
- [ ] Implementar `scripts/backup_database.py` (~2h)
  ```python
  # Funcionalidades:
  # - Copiar DB a backups/ con timestamp
  # - Comprimir con gzip
  # - Verificar integridad (PRAGMA integrity_check)
  # - Limpiar backups > 30 d√≠as
  ```
- [ ] Integrar en run_scheduler.py (~30min)
- [ ] Agregar a .gitignore (~5min)
  ```
  backups/*.db
  backups/*.gz
  ```
- [ ] Documentar proceso de restore (~30min)
- [ ] Test de backup y restore (~1h)
- [ ] Documentar en CHANGELOG.md (~15min)

### Notas t√©cnicas
- SQLite permite copiar archivo mientras est√° en uso (WAL mode)
- Verificar integridad antes de comprimir: `PRAGMA integrity_check`
- Restore: `gunzip backup.db.gz && cp backup.db bumeran_scraping.db`
- Considerar backup a ubicaci√≥n externa (Google Drive, S3) como mejora futura

### Referencias
- SQLite backup: https://www.sqlite.org/backup.html

### Verificaci√≥n final
```bash
# Ejecutar backup manual:
python scripts/backup_database.py

# Verificar backup creado:
ls -lh backups/

# Verificar integridad del backup:
gunzip -k backups/bumeran_scraping_2025-12-03.db.gz
sqlite3 backups/bumeran_scraping_2025-12-03.db "PRAGMA integrity_check"
# Output esperado: ok
```

---

## MOL-23: Versionado de datos

### Contexto
Actualmente solo se versiona c√≥digo (git), pero no datasets. Esto causa:
- No se puede reproducir m√©tricas hist√≥ricas
- Si ESCO se actualiza, no hay baseline de comparaci√≥n
- Gold sets cambian sin tracking

Referencia: Principios FAIR, mejores pr√°cticas MLOps

### Objetivo
Implementar versionado de datasets cr√≠ticos (gold sets, snapshots de BD, configuraciones).

### Archivos involucrados
- `data/gold_sets/` - NUEVO directorio
- `data/snapshots/` - NUEVO directorio
- `data/LATEST.json` - NUEVO (punteros a versiones activas)
- `scripts/version_data.py` - NUEVO script

### Criterios de Aceptaci√≥n
- [ ] Estructura de directorios creada
- [ ] Gold sets versionados: `data/gold_sets/gold_set_v1_2025-11-28.json`
- [ ] Snapshots de BD: `data/snapshots/ofertas_2025-12-03.csv.gz`
- [ ] Archivo LATEST.json apunta a versiones activas
- [ ] Script para crear nueva versi√≥n de gold set
- [ ] Script para crear snapshot
- [ ] Documentaci√≥n de uso

### Subtareas
- [ ] Crear estructura de directorios (~15min)
  ```
  data/
  ‚îú‚îÄ‚îÄ gold_sets/
  ‚îÇ   ‚îú‚îÄ‚îÄ gold_set_v1_2025-11-28.json
  ‚îÇ   ‚îî‚îÄ‚îÄ LATEST -> gold_set_v1_2025-11-28.json
  ‚îú‚îÄ‚îÄ snapshots/
  ‚îÇ   ‚îî‚îÄ‚îÄ ofertas_2025-12-03.csv.gz
  ‚îî‚îÄ‚îÄ LATEST.json
  ```
- [ ] Migrar gold set actual (~30min)
- [ ] Implementar version_data.py (~2h)
  - `--new-gold-set`: crea nueva versi√≥n de gold set
  - `--snapshot`: crea snapshot de ofertas
  - `--list`: lista versiones disponibles
- [ ] Actualizar test_gold_set_manual.py para usar LATEST (~30min)
- [ ] Documentar en README (~30min)
- [ ] Documentar en CHANGELOG.md (~15min)

### Notas t√©cnicas
- No usar DVC por complejidad, soluci√≥n simple con archivos
- LATEST.json estructura:
  ```json
  {
    "gold_set": "gold_sets/gold_set_v2_2025-12-15.json",
    "ofertas_snapshot": "snapshots/ofertas_2025-12-03.csv.gz",
    "esco_version": "1.1.2"
  }
  ```
- Snapshots solo de ofertas (la BD completa es muy grande)

### Referencias
- FAIR Principles: https://www.go-fair.org/fair-principles/
- `docs/ARCHITECTURE.md`

### Verificaci√≥n final
```bash
# Crear snapshot:
python scripts/version_data.py --snapshot

# Listar versiones:
python scripts/version_data.py --list

# Verificar LATEST:
cat data/LATEST.json
```

---

# üü° MEDIA PRIORIDAD

---

## MOL-7: M√©tricas Recall y F1

### Contexto
Actualmente solo medimos precisi√≥n (% de matches correctos). Un sistema podr√≠a tener 100% precisi√≥n si solo confirma 10 ofertas obvias. Necesitamos recall para saber qu√© % del universo estamos clasificando correctamente.

Referencia: `docs/TICKETS_CONTEXT.md#mol-7`

### Objetivo
Agregar m√©tricas de Recall, F1-Score y Top-3 Accuracy al benchmark.

### Archivos involucrados
- `database/test_gold_set_manual.py` - Modificar
- `database/gold_set_manual_v2.json` - Requiere campo `esco_esperado_uri`

### Criterios de Aceptaci√≥n
- [ ] Benchmark reporta Precision, Recall, F1-Score
- [ ] Benchmark reporta Top-3 Accuracy
- [ ] Gold set tiene `esco_esperado_uri` para cada caso
- [ ] Output formateado:
  ```
  === M√âTRICAS COMPLETAS ===
  Precision:   80.0%
  Recall:      75.0%
  F1-Score:    77.4%
  Top-3 Acc:   92.0%
  ```
- [ ] Documentado en CHANGELOG.md

### Subtareas
- [ ] Agregar `esco_esperado_uri` a gold set (~requiere MOL-6)
- [ ] Implementar c√°lculo de Recall (~1h)
- [ ] Implementar c√°lculo de F1 (~30min)
- [ ] Implementar Top-3 Accuracy (~1h)
- [ ] Formatear output (~30min)
- [ ] Documentar (~30min)

### Notas t√©cnicas
- Recall = matches_correctos / total_con_esco_esperado
- Precision = matches_correctos / total_confirmados
- F1 = 2 * (P * R) / (P + R)
- Top-3: verificar si `esco_esperado` est√° en los top 3 candidatos

### Dependencias
- Requiere: MOL-6 (gold set expandido con esco_esperado_uri)

### Verificaci√≥n final
```bash
python database/test_gold_set_manual.py
# Output debe incluir: Precision, Recall, F1, Top-3
```

---

## MOL-8: Resolver casos biling√ºes

### Contexto
T√≠tulos en ingl√©s no matchean bien con ocupaciones ESCO en espa√±ol. Los embeddings de BGE-M3 son multiling√ºes pero la distancia sem√°ntica ingl√©s‚Üîespa√±ol a veces es mayor que la distancia a ocupaciones incorrectas.

### Objetivo
Mejorar matching de t√≠tulos en ingl√©s mediante traducci√≥n o diccionario de equivalencias.

### Archivos involucrados
- `database/title_translations.json` - NUEVO diccionario
- `database/match_ofertas_multicriteria.py` - Modificar preproceso

### Criterios de Aceptaci√≥n
- [ ] Diccionario con ‚â•50 traducciones comunes
- [ ] Preproceso traduce t√≠tulo antes de embedding
- [ ] "Account Executive" matchea a ventas
- [ ] "Software Developer" matchea a desarrollador
- [ ] "Data Analyst" matchea a analista de datos
- [ ] Sin regresiones en t√≠tulos en espa√±ol
- [ ] Documentado en CHANGELOG.md

### Subtareas
- [ ] Crear title_translations.json (~2h)
- [ ] Implementar preproceso de traducci√≥n (~1h)
- [ ] Validar con casos biling√ºes del gold set (~1h)
- [ ] Documentar (~30min)

### Notas t√©cnicas
- Alternativa a diccionario: usar API de traducci√≥n (pero agrega latencia)
- Priorizar t√©rminos de IT, ventas, marketing (m√°s comunes en ingl√©s)
- Formato diccionario:
  ```json
  {
    "account executive": "ejecutivo de cuentas",
    "software developer": "desarrollador de software",
    "data analyst": "analista de datos"
  }
  ```

### Verificaci√≥n final
```bash
# Test manual:
python -c "
from database.match_ofertas_multicriteria import preprocesar_titulo
print(preprocesar_titulo('Account Executive'))
# Output esperado: 'ejecutivo de cuentas'
"
```

---

## MOL-19: Pipeline autom√°tico post-scraping

### Contexto
Despu√©s del scraping hay que ejecutar manualmente: consolidaci√≥n ‚Üí NLP ‚Üí matching. Esto genera retrasos y posibles olvidos.

### Objetivo
Automatizar el pipeline completo post-scraping con un solo comando.

### Archivos involucrados
- `scripts/run_full_pipeline.py` - NUEVO script
- `run_scheduler.py` - Modificar (invocar pipeline)

### Criterios de Aceptaci√≥n
- [ ] Un comando ejecuta todo: `python scripts/run_full_pipeline.py`
- [ ] Orden correcto: consolidar ‚Üí NLP ‚Üí matching
- [ ] Si un paso falla, no ejecuta los siguientes
- [ ] Log de cada paso con tiempo de ejecuci√≥n
- [ ] Notificaci√≥n al terminar (ver MOL-14)
- [ ] Documentado

### Subtareas
- [ ] Crear run_full_pipeline.py (~2h)
- [ ] Integrar con run_scheduler.py (~1h)
- [ ] Implementar logging por paso (~1h)
- [ ] Test completo (~1h)
- [ ] Documentar (~30min)

### Dependencias
- Requiere: MOL-18 (scrapers automatizados)

### Verificaci√≥n final
```bash
python scripts/run_full_pipeline.py --dry-run
# Output: muestra pasos que ejecutar√≠a sin ejecutarlos
```

---

## MOL-14: Alertas email/Slack

### Contexto
`alert_manager.py` existe pero tiene `email_enabled=False`. No hay notificaciones cuando algo falla o cuando el pipeline termina.

### Objetivo
Habilitar alertas por email o Slack para eventos cr√≠ticos.

### Archivos involucrados
- `database/alert_manager.py` - Modificar
- `config/alerts_config.yaml` - NUEVO

### Criterios de Aceptaci√≥n
- [ ] Alerta cuando scraping falla
- [ ] Alerta cuando ofertas_nuevas < threshold
- [ ] Alerta cuando pipeline completo termina
- [ ] Resumen diario con m√©tricas
- [ ] Configuraci√≥n en YAML (no hardcodeada)
- [ ] Al menos un canal funcional (email o Slack)

### Subtareas
- [ ] Definir eventos que disparan alertas (~30min)
- [ ] Crear alerts_config.yaml (~30min)
- [ ] Implementar env√≠o de email (~2h)
- [ ] Implementar webhook de Slack (alternativo) (~2h)
- [ ] Integrar con run_scheduler.py (~1h)
- [ ] Test de alertas (~30min)
- [ ] Documentar (~30min)

### Notas t√©cnicas
- Email: usar smtplib con Gmail o SendGrid
- Slack: webhook simple, no requiere bot
- Considerar rate limiting de alertas (m√°ximo 3/hora)

### Verificaci√≥n final
```bash
# Test de alerta:
python -c "from database.alert_manager import send_alert; send_alert('Test', 'Mensaje de prueba')"
```

---

## MOL-16: Fix shinyTree

### Contexto
El componente shinyTree para navegar la jerarqu√≠a ESCO est√° deshabilitado por un bug de input/output binding.

### Objetivo
Rehabilitar el √°rbol ESCO navegable en el dashboard.

### Archivos involucrados
- `Visual--/app.R` - Modificar
- `Visual--/www/custom.js` - Posible fix de bindings

### Criterios de Aceptaci√≥n
- [ ] √Årbol ESCO visible y navegable
- [ ] Al seleccionar ocupaci√≥n, filtran ofertas
- [ ] Sin errores en consola R
- [ ] Sin errores en consola browser

### Subtareas
- [ ] Debuggear error actual (~1h)
- [ ] Implementar fix (~2h)
- [ ] Test en diferentes browsers (~30min)
- [ ] Documentar (~15min)

### Notas t√©cnicas
- Error conocido: "Error in shinyTree: input binding not found"
- Posible causa: versi√≥n de shinyTree incompatible
- Alternativa: usar otro widget de √°rbol (jsTree, collapsibleTree)

### Verificaci√≥n final
```r
# En R:
shiny::runApp('Visual--/app.R', port=3853)
# Verificar que el √°rbol carga y es interactivo
```

---

## MOL-24: Entity Resolution cross-source

### Contexto
Cuando se automaticen los 5 scrapers, la misma oferta puede aparecer en m√∫ltiples portales. Actualmente no hay deduplicaci√≥n cross-source.

### Objetivo
Implementar detecci√≥n de duplicados entre diferentes fuentes.

### Archivos involucrados
- `02_consolidation/scripts/deduplicacion_cross_source.py` - NUEVO
- `database/bumeran_scraping.db` - Agregar tabla `ofertas_canonical`

### Criterios de Aceptaci√≥n
- [ ] Detecci√≥n de duplicados basada en (titulo + empresa + ubicacion)
- [ ] Hash de deduplicaci√≥n: `canonical_id`
- [ ] Tabla `ofertas_canonical` con oferta representativa
- [ ] M√©tricas: % de duplicados detectados
- [ ] Sin falsos positivos (ofertas diferentes marcadas como iguales)

### Subtareas
- [ ] Dise√±ar algoritmo de hash (~1h)
- [ ] Crear tabla ofertas_canonical (~30min)
- [ ] Implementar script (~3h)
- [ ] Validar con muestra manual (~1h)
- [ ] Documentar (~30min)

### Notas t√©cnicas
- Hash: normalizar t√≠tulo (lowercase, sin puntuaci√≥n) + normalizar empresa + c√≥digo provincia
- Usar Levenshtein para fuzzy matching de t√≠tulos
- Threshold de similaridad: >0.85

### Dependencias
- Requiere: MOL-18 (tener datos de m√∫ltiples fuentes)

### Verificaci√≥n final
```sql
-- Verificar duplicados detectados:
SELECT canonical_id, COUNT(*) as fuentes
FROM ofertas
WHERE canonical_id IS NOT NULL
GROUP BY canonical_id
HAVING COUNT(*) > 1
```

---

## MOL-25: Drift Detection

### Contexto
Si un portal cambia su API/HTML, el scraper puede fallar silenciosamente (capturar 0 ofertas o datos corruptos). Actualmente no hay monitoreo de esto.

### Objetivo
Implementar detecci√≥n de anomal√≠as en el proceso de scraping.

### Archivos involucrados
- `scripts/drift_detector.py` - NUEVO
- `config/drift_thresholds.yaml` - NUEVO

### Criterios de Aceptaci√≥n
- [ ] Alerta si ofertas_nuevas < threshold diario
- [ ] Alerta si campos obligatorios vienen vac√≠os (>10%)
- [ ] Alerta si estructura de respuesta cambia
- [ ] Health check validable manualmente
- [ ] Integrado con sistema de alertas (MOL-14)

### Subtareas
- [ ] Definir m√©tricas de drift (~1h)
- [ ] Implementar drift_detector.py (~2h)
- [ ] Definir thresholds por fuente (~1h)
- [ ] Integrar con alertas (~1h)
- [ ] Documentar (~30min)

### Notas t√©cnicas
- Threshold ofertas: Bumeran >50/d√≠a, ZonaJobs >30/d√≠a
- Campos obligatorios: titulo, empresa, descripcion
- Guardar historial de m√©tricas para detectar tendencias

### Verificaci√≥n final
```bash
python scripts/drift_detector.py --check-all
# Output: OK/WARNING/CRITICAL por fuente
```

---

# ‚ö™ BAJA PRIORIDAD

---

## MOL-9: CI/CD GitHub Actions

### Contexto
No hay validaci√≥n autom√°tica de c√≥digo. Un push puede romper el matching sin que nadie se entere hasta ejecutar manualmente.

### Objetivo
Configurar GitHub Actions para validar autom√°ticamente en cada push.

### Archivos involucrados
- `.github/workflows/test.yml` - NUEVO

### Criterios de Aceptaci√≥n
- [ ] Workflow ejecuta en cada push a main
- [ ] Corre test_gold_set_manual.py
- [ ] Falla si precision < 75%
- [ ] Badge de estado en README

### Subtareas
- [ ] Crear workflow YAML (~1h)
- [ ] Configurar secrets si necesario (~30min)
- [ ] Agregar badge a README (~15min)
- [ ] Test del workflow (~30min)

### Dependencias
- Requiere: MOL-5 estable (precisi√≥n consistente)

### Verificaci√≥n final
```bash
# Push a main y verificar que Action corre
git push origin main
# Verificar en GitHub Actions tab
```

---

## MOL-10: Regex abreviaciones argentinas

### Contexto
El regex v4 no detecta abreviaciones comunes argentinas: Adm., Gte., Coord., Jfe., Aux.

### Objetivo
Agregar patrones para abreviaciones argentinas en regex v4.1.

### Archivos involucrados
- `02.5_nlp_extraction/scripts/patterns/regex_patterns_v4.py` - Modificar (crear v4.1)

### Criterios de Aceptaci√≥n
- [ ] Detecta: Adm., Gte., Coord., Jfe., Aux., Ing., Lic., Dr.
- [ ] Expande a forma completa
- [ ] Sin falsos positivos
- [ ] Cobertura regex aumenta (60-70% ‚Üí 70-75%)

### Subtareas
- [ ] Listar abreviaciones comunes (~30min)
- [ ] Implementar patrones (~1h)
- [ ] Test con muestra (~1h)
- [ ] Documentar (~30min)

### Verificaci√≥n final
```python
from regex_patterns_v4 import extraer_nivel
assert extraer_nivel("Gte. Comercial") == "gerente"
```

---

## MOL-11: Niveles jer√°rquicos

### Contexto
La detecci√≥n de niveles (Junior/Senior/Manager) tiene baja cobertura. Muchas ofertas no explicitan el nivel.

### Objetivo
Mejorar inferencia de nivel jer√°rquico desde contexto.

### Archivos involucrados
- `02.5_nlp_extraction/scripts/patterns/regex_patterns_v4.py` - Modificar
- `database/matching_rules_v84.py` - Ajustar penalizaciones

### Criterios de Aceptaci√≥n
- [ ] Detecta nivel desde a√±os de experiencia requeridos
- [ ] Detecta nivel desde salario (si disponible)
- [ ] Cobertura de nivel aumenta

### Subtareas
- [ ] Mapear experiencia ‚Üí nivel (~1h)
- [ ] Implementar inferencia (~2h)
- [ ] Validar (~1h)
- [ ] Documentar (~30min)

---

## MOL-12: Consolidar NLP v6+v7

### Contexto
Existen m√∫ltiples versiones de scripts NLP. Dificulta mantenimiento.

### Objetivo
Unificar en un solo archivo con flags de configuraci√≥n.

### Archivos involucrados
- `database/process_nlp_unified.py` - NUEVO
- `database/process_nlp_from_db_v6.py` - Deprecar
- `database/process_nlp_from_db_v7.py` - Deprecar

### Criterios de Aceptaci√≥n
- [ ] Un solo script: process_nlp_unified.py
- [ ] Flag --version para seleccionar comportamiento
- [ ] Versiones antiguas movidas a archive/
- [ ] Sin regresiones

### Subtareas
- [ ] Analizar diferencias v6 vs v7 (~1h)
- [ ] Implementar script unificado (~3h)
- [ ] Test de equivalencia (~1h)
- [ ] Deprecar versiones antiguas (~30min)

---

## MOL-13: Panel administraci√≥n

### Contexto
No existe panel de administraci√≥n. Todo es CLI.

### Objetivo
Crear panel web para monitorear y operar el sistema.

### Archivos involucrados
- `admin/admin_panel.py` - NUEVO (Dash app, puerto 8053)

### Criterios de Aceptaci√≥n
- [ ] Ver estado de scrapers
- [ ] Ver estado de pipeline NLP/Matching
- [ ] Ejecutar tareas manualmente
- [ ] Ver logs recientes
- [ ] Ver m√©tricas gold set

### Dependencias
- Requiere: MOL-20 (logs centralizados)

---

## MOL-15: Limpieza JSONs

### Contexto
10,800 archivos JSON en 01_sources/*/data/raw/, muchos duplicados.

### Objetivo
Limpiar JSONs antiguos y consolidar.

### Criterios de Aceptaci√≥n
- [ ] Eliminar JSONs > 30 d√≠as ya procesados
- [ ] Liberar >1GB de espacio
- [ ] Script de limpieza automatizable

---

## MOL-17: Auth shinymanager

### Contexto
Autenticaci√≥n deshabilitada para debug. Dashboard p√∫blicamente accesible.

### Objetivo
Rehabilitar autenticaci√≥n con shinymanager.

### Dependencias
- Requiere: MOL-16 (estabilidad del dashboard)

---

## MOL-20: Centralizar logs

### Contexto
Logs distribuidos en m√∫ltiples carpetas.

### Objetivo
Todos los logs en `logs/` con formato unificado.

### Criterios de Aceptaci√≥n
- [ ] Directorio √∫nico: logs/
- [ ] Rotaci√≥n autom√°tica (7 d√≠as)
- [ ] Formato: `[timestamp] [nivel] [m√≥dulo] mensaje`

---

## MOL-21: Deprecar dashboards

### Contexto
M√∫ltiples versiones de dashboards obsoletos.

### Objetivo
Mantener solo 2 dashboards activos, mover resto a archive/.

---

## MOL-22: Documentar APIs

### Contexto
APIs de scrapers descubiertas por reverse engineering, sin documentaci√≥n.

### Objetivo
Documentar endpoints, headers, payloads, rate limits.

### Archivos involucrados
- `docs/SCRAPER_APIS.md` - NUEVO

---

# Ap√©ndice: Grafo de Dependencias

```
                         ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
                         ‚îÇ  MOL-6  ‚îÇ Gold Set 50+
                         ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                              ‚îÇ mejora
            ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
            ‚ñº                 ‚ñº                 ‚ñº
      ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê       ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê       ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
      ‚îÇ  MOL-5  ‚îÇ       ‚îÇ  MOL-7  ‚îÇ       ‚îÇ  MOL-8  ‚îÇ
      ‚îÇsector_fn‚îÇ       ‚îÇ Recall  ‚îÇ       ‚îÇbiling√ºe ‚îÇ
      ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îò       ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò       ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
           ‚îÇ estabiliza
           ‚ñº
      ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
      ‚îÇ  MOL-9  ‚îÇ CI/CD
      ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò

      ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê       ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
      ‚îÇ MOL-18  ‚îÇ ‚îÄ‚îÄ‚îÄ‚îÄ‚ñ∫ ‚îÇ MOL-19  ‚îÇ
      ‚îÇscrapers ‚îÇ       ‚îÇpipeline ‚îÇ
      ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îò       ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îò
           ‚îÇ                 ‚îÇ
           ‚ñº                 ‚ñº
      ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê       ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
      ‚îÇ MOL-24  ‚îÇ       ‚îÇ MOL-14  ‚îÇ
      ‚îÇ dedup   ‚îÇ       ‚îÇalertas  ‚îÇ
      ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò       ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò

      ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê       ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
      ‚îÇ MOL-20  ‚îÇ ‚îÄ‚îÄ‚îÄ‚îÄ‚ñ∫ ‚îÇ MOL-13  ‚îÇ
      ‚îÇ  logs   ‚îÇ       ‚îÇ  admin  ‚îÇ
      ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò       ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò

      ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê       ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
      ‚îÇ MOL-16  ‚îÇ ‚îÄ‚îÄ‚îÄ‚îÄ‚ñ∫ ‚îÇ MOL-17  ‚îÇ
      ‚îÇshinyTree‚îÇ       ‚îÇ  auth   ‚îÇ
      ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò       ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

---

> **Generado:** 2025-12-03
> **Formato:** Basado en CCT-RAG issue template
