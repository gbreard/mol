# üöÄ DEPLOYMENT - Sistema de Scraping Bumeran en Producci√≥n

**Versi√≥n:** 4.0 - Sistema Completo con Automatizaci√≥n
**Fecha:** 30 Octubre 2025
**Estado:** ‚úÖ Listo para Producci√≥n

---

## üìã Tabla de Contenidos

1. [Resumen del Sistema](#resumen-del-sistema)
2. [Requisitos del Sistema](#requisitos-del-sistema)
3. [Instalaci√≥n Inicial](#instalaci√≥n-inicial)
4. [Configuraci√≥n de PostgreSQL](#configuraci√≥n-de-postgresql)
5. [Configuraci√≥n de Variables de Entorno](#configuraci√≥n-de-variables-de-entorno)
6. [Ejecuci√≥n Manual (Testing)](#ejecuci√≥n-manual-testing)
7. [Automatizaci√≥n Semanal](#automatizaci√≥n-semanal)
8. [Dashboard de M√©tricas](#dashboard-de-m√©tricas)
9. [Monitoreo y Mantenimiento](#monitoreo-y-mantenimiento)
10. [Troubleshooting](#troubleshooting)

---

## üéØ Resumen del Sistema

### Componentes Implementados

**Fase 1-3: Scraper Robusto** ‚úÖ
- ‚úÖ Tracking incremental seguro (operaciones at√≥micas)
- ‚úÖ Retry autom√°tico con exponential backoff (tenacity)
- ‚úÖ Validaci√≥n de schemas (Pydantic)
- ‚úÖ Fechas ISO 8601 con timezone Argentina
- ‚úÖ Limpieza de HTML entities
- ‚úÖ Sistema de m√©tricas de performance
- ‚úÖ Rate limiting adaptativo (0.5s-10s)
- ‚úÖ Circuit breaker (fail-fast tras 5 fallos)
- ‚úÖ Sistema de alertas autom√°ticas

**Fase 4: Infraestructura de Producci√≥n** ‚úÖ
- ‚úÖ Base de datos PostgreSQL (5 tablas, 3 vistas)
- ‚úÖ DatabaseManager (db_manager.py)
- ‚úÖ Scheduler automatizado (lunes y jueves 8:00 AM)
- ‚úÖ Scripts de deployment (batch files)
- ‚è≥ Dashboard Plotly Dash (pendiente)

### Arquitectura

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                    SISTEMA DE SCRAPING                       ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ                                                               ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê      ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê‚îÇ
‚îÇ  ‚îÇ   Bumeran    ‚îÇ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÇ    Scraper    ‚îÇ‚îÄ‚îÄ‚îÄ‚îÄ‚îÇ  PostgreSQL ‚îÇ‚îÇ
‚îÇ  ‚îÇ     API      ‚îÇ      ‚îÇ   (Fases 1-3) ‚îÇ    ‚îÇ             ‚îÇ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò      ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò‚îÇ
‚îÇ                               ‚îÇ                      ‚îÇ       ‚îÇ
‚îÇ                               ‚îÇ                      ‚îÇ       ‚îÇ
‚îÇ                      ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê           ‚îÇ       ‚îÇ
‚îÇ                      ‚îÇ   Optimizaciones  ‚îÇ           ‚îÇ       ‚îÇ
‚îÇ                      ‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§           ‚îÇ       ‚îÇ
‚îÇ                      ‚îÇ ‚Ä¢ Rate Limiter    ‚îÇ           ‚îÇ       ‚îÇ
‚îÇ                      ‚îÇ ‚Ä¢ Circuit Breaker ‚îÇ           ‚îÇ       ‚îÇ
‚îÇ                      ‚îÇ ‚Ä¢ Alertas         ‚îÇ           ‚îÇ       ‚îÇ
‚îÇ                      ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò           ‚îÇ       ‚îÇ
‚îÇ                               ‚îÇ                      ‚îÇ       ‚îÇ
‚îÇ                      ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê           ‚îÇ       ‚îÇ
‚îÇ                      ‚îÇ   CSV + Tracking  ‚îÇ           ‚îÇ       ‚îÇ
‚îÇ                      ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò           ‚îÇ       ‚îÇ
‚îÇ                                                       ‚îÇ       ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê‚îÇ
‚îÇ  ‚îÇ              Scheduler (Lunes y Jueves 8:00 AM)         ‚îÇ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò‚îÇ
‚îÇ                                                               ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê‚îÇ
‚îÇ  ‚îÇ         Dashboard Plotly Dash (localhost:8050)          ‚îÇ‚îÇ
‚îÇ  ‚îÇ  ‚Ä¢ M√©tricas hist√≥ricas  ‚Ä¢ Circuit breaker stats         ‚îÇ‚îÇ
‚îÇ  ‚îÇ  ‚Ä¢ Rate limiter         ‚Ä¢ Alertas                       ‚îÇ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

---

## üíª Requisitos del Sistema

### Software Necesario

1. **Python 3.9+**
   - Descargar: https://www.python.org/downloads/
   - Asegurarse de agregar Python a PATH durante instalaci√≥n

2. **PostgreSQL 14+**
   - Descargar: https://www.postgresql.org/download/
   - Durante instalaci√≥n, recordar password de usuario `postgres`

3. **Git** (opcional, para control de versiones)
   - Descargar: https://git-scm.com/downloads

### Hardware Recomendado

- **CPU:** 2+ cores
- **RAM:** 4+ GB
- **Disco:** 10+ GB libres
- **Internet:** Conexi√≥n estable

---

## üîß Instalaci√≥n Inicial

### Paso 1: Clonar/Descargar Proyecto

```bash
cd D:\OEDE\Webscrapping
```

### Paso 2: Instalar Dependencias de Python

```bash
# Crear entorno virtual (recomendado)
python -m venv venv

# Activar entorno virtual
# Windows:
venv\Scripts\activate
# Linux/Mac:
source venv/bin/activate

# Instalar dependencias
pip install -r config\requirements.txt
```

**Dependencias incluidas:**
- `requests` - HTTP requests
- `pandas` - Procesamiento de datos
- `tenacity` - Retry logic
- `pydantic` - Validaci√≥n de schemas
- `psycopg2-binary` - PostgreSQL driver
- `schedule` - Scheduler
- `plotly`, `dash` - Dashboard (pendiente)

### Paso 3: Verificar Instalaci√≥n

```bash
python --version  # Debe mostrar Python 3.9+
pip list | findstr postgres  # Debe mostrar psycopg2
pip list | findstr schedule  # Debe mostrar schedule
```

---

## üóÑÔ∏è Configuraci√≥n de PostgreSQL

### Paso 1: Iniciar PostgreSQL

**Windows:**
- Buscar "pgAdmin 4" en el men√∫ inicio
- O verificar que el servicio `postgresql-x64-14` est√© corriendo en Services

**Verificar conexi√≥n:**
```bash
psql -U postgres -c "SELECT version();"
```

### Paso 2: Crear Base de Datos

**Opci√≥n A: Usando pgAdmin (GUI)**
1. Abrir pgAdmin 4
2. Conectar a servidor local (usuario: postgres, password: tu_password)
3. Click derecho en "Databases" ‚Üí Create ‚Üí Database
4. Nombre: `bumeran_scraping`
5. Encoding: UTF8
6. OK

**Opci√≥n B: Usando psql (Command Line)**
```bash
cd D:\OEDE\Webscrapping\database

# Ejecutar script de creaci√≥n
psql -U postgres -f create_database.sql

# Deber√≠a mostrar:
# ‚úÖ Base de datos creada exitosamente!
```

### Paso 3: Verificar Tablas Creadas

```sql
-- Conectar a la base de datos
psql -U postgres -d bumeran_scraping

-- Listar tablas
\dt

-- Deber√≠a mostrar:
-- ofertas
-- metricas_scraping
-- alertas
-- circuit_breaker_stats
-- rate_limiter_stats
```

---

## üîê Configuraci√≥n de Variables de Entorno

### Paso 1: Crear archivo .env

```bash
cd D:\OEDE\Webscrapping

# Copiar plantilla
copy .env.example .env

# Editar con tu editor favorito
notepad .env
```

### Paso 2: Configurar Credenciales

Editar `.env` con tus valores:

```bash
# PostgreSQL
DB_HOST=localhost
DB_PORT=5432
DB_NAME=bumeran_scraping
DB_USER=postgres
DB_PASSWORD=TU_PASSWORD_AQUI  # ‚Üê CAMBIAR!

# Alertas (Opcional - Futuro)
ALERT_EMAIL=tu_email@example.com
```

### Paso 3: Verificar Configuraci√≥n

```bash
cd D:\OEDE\Webscrapping

python database\config.py
```

Deber√≠a mostrar:
```
======================================================================
CONFIGURACI√ìN ACTUAL
======================================================================

BASE DE DATOS:
  Host:     localhost
  Port:     5432
  Database: bumeran_scraping
  User:     postgres
  Password: ******* (configurado)

SCHEDULER:
  D√≠as:     Lun, Jue
  Hora:     08:00
  Timezone: America/Argentina/Buenos_Aires

‚úì Configuraci√≥n v√°lida
```

---

## üß™ Ejecuci√≥n Manual (Testing)

### Test 1: Scraping Peque√±o (Recomendado Primero)

```bash
cd D:\OEDE\Webscrapping\01_sources\bumeran\scrapers

# Test de Fase 1 (tracking, retry, validaci√≥n)
python test_fase1_mejoras.py

# Test de Fase 2 (fechas ISO, HTML, m√©tricas)
python test_fase2_mejoras.py

# Test de Fase 3 (rate limiter, circuit breaker, alertas)
python test_fase3_mejoras.py
```

**Resultado esperado:** `[PASS] TEST X COMPLETADO` para cada test

### Test 2: Scraping Completo (12k ofertas)

```bash
cd D:\OEDE\Webscrapping\01_sources\bumeran\scrapers

python run_scraping_completo.py
```

**Duraci√≥n estimada:** 15-25 minutos (dependiendo de rate limiter adaptativo)

**Resultado esperado:**
```
======================================================================
SCRAPING COMPLETADO EXITOSAMENTE - 12,142 OFERTAS
======================================================================

Archivos generados:
  CSV     : D:\OEDE\Webscrapping\01_sources\bumeran\data\raw\bumeran_completo_20251030_123456.csv
  JSON    : D:\OEDE\Webscrapping\01_sources\bumeran\data\raw\bumeran_completo_20251030_123456.json
  EXCEL   : D:\OEDE\Webscrapping\01_sources\bumeran\data\raw\bumeran_completo_20251030_123456.xlsx
```

### Test 3: Integraci√≥n con PostgreSQL

```bash
cd D:\OEDE\Webscrapping

# Test de scheduler (ejecuta scraping inmediatamente)
python run_scheduler.py --test
```

**Resultado esperado:**
```
TEST MODE - Ejecutando scraping inmediatamente
======================================================================
INICIANDO SCRAPING PROGRAMADO
======================================================================
...
Insertando 12,142 ofertas...
‚úì 12,142 ofertas insertadas/actualizadas
M√©tricas guardadas con ID: 1
Total ofertas en DB: 12,142
======================================================================
SCRAPING COMPLETADO EXITOSAMENTE - 15.35 minutos
======================================================================
```

---

## ‚è∞ Automatizaci√≥n Semanal

### Opci√≥n A: Scheduler Python (Recomendado)

**Ventaja:** Simple, portable, logs detallados

#### Paso 1: Iniciar Scheduler

**Modo Visible (para testing):**
```bash
cd D:\OEDE\Webscrapping
start_scheduler.bat
```

**Modo Oculto (producci√≥n):**
```bash
cd D:\OEDE\Webscrapping
start_scheduler.bat --hidden
```

#### Paso 2: Verificar Scheduler Est√° Corriendo

**Task Manager:**
- Presionar `Ctrl + Shift + Esc`
- Buscar proceso `python.exe` o `pythonw.exe`
- Deber√≠a estar corriendo `run_scheduler.py`

**Logs:**
```bash
# Ver logs en tiempo real
cd D:\OEDE\Webscrapping\logs
type scheduler_202510.log
```

#### Paso 3: Detener Scheduler

**Si modo visible:** Presionar `Ctrl + C` en la consola

**Si modo oculto:**
- Task Manager ‚Üí buscar `pythonw.exe` ‚Üí End Task

---

### Opci√≥n B: Windows Task Scheduler (M√°s Robusto)

**Ventaja:** Sobrevive a reinicios, ejecuci√≥n garantizada

#### Paso 1: Abrir Task Scheduler

1. Presionar `Win + R`
2. Escribir `taskschd.msc`
3. Enter

#### Paso 2: Crear Nueva Tarea

1. Click derecho en "Task Scheduler Library" ‚Üí Create Task...
2. **General Tab:**
   - Name: `Bumeran Scraping - Lunes`
   - Description: `Scraping automatizado de Bumeran (Lunes 8:00 AM)`
   - Run whether user is logged on or not: ‚úì
   - Run with highest privileges: ‚úì

3. **Triggers Tab:**
   - New...
   - Begin the task: `On a schedule`
   - Weekly
   - Start: `08:00:00`
   - Recur every: `1 weeks on`
   - Monday: ‚úì
   - OK

4. **Actions Tab:**
   - New...
   - Action: `Start a program`
   - Program/script: `C:\Path\To\Python\python.exe`
     (Encontrar con: `where python`)
   - Add arguments: `D:\OEDE\Webscrapping\run_scheduler.py --test`
   - Start in: `D:\OEDE\Webscrapping`
   - OK

5. **Conditions Tab:**
   - Uncheck: `Start the task only if the computer is on AC power`

6. **Settings Tab:**
   - Allow task to be run on demand: ‚úì
   - Run task as soon as possible after a scheduled start is missed: ‚úì

#### Paso 3: Repetir para Jueves

Crear otra tarea id√©ntica pero con trigger en **Thursday**

#### Paso 4: Probar Tarea

Click derecho en tarea ‚Üí Run

Verificar logs en `D:\OEDE\Webscrapping\logs\`

---

## üìä Dashboard de M√©tricas

**Estado:** ‚è≥ Pendiente de implementaci√≥n

**Pr√≥ximos pasos:**
1. Crear `dashboard_app.py` con Plotly Dash
2. Gr√°ficos:
   - Evoluci√≥n delay adaptativo
   - Circuit breaker opens timeline
   - Tasas de validaci√≥n
   - Ofertas scrapeadas por ejecuci√≥n
3. Ejecuci√≥n: `python dashboard_app.py`
4. Acceso: http://localhost:8050

---

## üîç Monitoreo y Mantenimiento

### Logs

**Ubicaci√≥n:** `D:\OEDE\Webscrapping\logs\`

**Archivos:**
- `scheduler_YYYYMM.log` - Log del scheduler (rotaci√≥n mensual)
- `scraping_YYYYMMDD_HHMMSS.log` - Log de cada ejecuci√≥n manual

**Ver logs recientes:**
```bash
cd D:\OEDE\Webscrapping\logs

# √öltimas 50 l√≠neas
powershell Get-Content scheduler_202510.log -Tail 50

# En tiempo real
powershell Get-Content scheduler_202510.log -Wait
```

### Verificar M√©tricas en PostgreSQL

```sql
-- Conectar
psql -U postgres -d bumeran_scraping

-- √öltimas 10 ejecuciones
SELECT * FROM v_ultimas_ejecuciones;

-- Alertas cr√≠ticas recientes
SELECT * FROM v_alertas_criticas LIMIT 10;

-- Total ofertas
SELECT COUNT(*) FROM ofertas;
```

### Backups

**Base de Datos (Semanal Recomendado):**
```bash
cd D:\OEDE\Webscrapping\backups

# Crear backup
pg_dump -U postgres -d bumeran_scraping -F c -f bumeran_scraping_20251030.backup

# Restaurar (si es necesario)
pg_restore -U postgres -d bumeran_scraping -c bumeran_scraping_20251030.backup
```

**CSV (Autom√°tico):**
Los archivos CSV se guardan autom√°ticamente en:
`D:\OEDE\Webscrapping\01_sources\bumeran\data\raw\`

### Limpieza de Logs Antiguos

```bash
# Eliminar logs >3 meses
cd D:\OEDE\Webscrapping\logs
del /Q scheduler_202407.log
del /Q scheduler_202408.log
```

---

## üêõ Troubleshooting

### Error: "No module named 'psycopg2'"

**Causa:** PostgreSQL driver no instalado

**Soluci√≥n:**
```bash
pip install psycopg2-binary
```

### Error: "password authentication failed for user postgres"

**Causa:** Password incorrecta en `.env`

**Soluci√≥n:**
1. Verificar password en pgAdmin
2. Actualizar `DB_PASSWORD` en `.env`
3. Reintentar

### Error: "CRITICAL: >50% de ofertas inv√°lidas. ¬øCambi√≥ el schema de la API?"

**Causa:** Bumeran cambi√≥ el formato de su API

**Soluci√≥n:**
1. Verificar respuesta de API manualmente:
   ```bash
   curl -X POST https://www.bumeran.com.ar/api/avisos/searchV2 \
     -H "x-site-id: BMAR" -H "Content-Type: application/json" \
     -d '{"page": 0, "pageSize": 1}'
   ```
2. Actualizar `bumeran_schemas.py` con nuevo formato
3. Reintentar

### Scraping Muy Lento

**Causa:** Rate limiter demasiado conservador

**Soluci√≥n:**
Editar `database/config.py`:
```python
'rate_limiter': {
    'min_delay': 0.3,  # Reducir de 0.5s
    'max_delay': 10.0,
    'success_threshold': 3,  # Reducir de 5
}
```

### Circuit Breaker Se Abre Frecuentemente

**Causa:** API inestable o max_failures muy bajo

**Soluci√≥n:**
Editar `database/config.py`:
```python
'circuit_breaker': {
    'max_failures': 10,  # Aumentar de 5
    'timeout': 60,  # Aumentar de 30s
}
```

### Scheduler No Ejecuta a Horario Programado

**Causa:** M√∫ltiples posibles

**Soluci√≥n:**
1. Verificar zona horaria:
   ```python
   from datetime import datetime
   import pytz

   tz = pytz.timezone('America/Argentina/Buenos_Aires')
   print(datetime.now(tz))
   ```
2. Verificar scheduler est√° corriendo:
   ```bash
   tasklist | findstr python
   ```
3. Revisar logs para errores

---

## üìû Soporte

**Documentaci√≥n Adicional:**
- `MEJORAS_FASE1_COMPLETADAS.md` - Tracking + Retry + Validaci√≥n
- `MEJORAS_FASE2_COMPLETADAS.md` - Fechas + HTML + M√©tricas
- `MEJORAS_FASE3_COMPLETADAS.md` - Rate Limiter + Circuit Breaker + Alertas

**Archivos de Configuraci√≥n:**
- `database/config.py` - Configuraci√≥n centralizada
- `.env` - Variables de entorno (NO subir a Git)

**Scripts √ötiles:**
- `run_scraping_completo.py` - Scraping manual completo
- `run_scheduler.py` - Scheduler automatizado
- `start_scheduler.bat` - Inicio r√°pido del scheduler
- `database/db_manager.py` - Gestor de base de datos

---

## ‚úÖ Checklist de Deployment

Antes de poner en producci√≥n, verificar:

- [ ] PostgreSQL instalado y corriendo
- [ ] Base de datos `bumeran_scraping` creada
- [ ] Tablas creadas (5 tablas, 3 vistas)
- [ ] Python 3.9+ instalado
- [ ] Dependencias instaladas (`pip install -r requirements.txt`)
- [ ] Archivo `.env` configurado con password correcto
- [ ] Tests de Fase 1+2+3 pasando
- [ ] Test de scraping completo exitoso
- [ ] Test de integraci√≥n PostgreSQL exitoso
- [ ] Scheduler funcionando (al menos 1 ejecuci√≥n exitosa)
- [ ] Logs configurados y monitoreados
- [ ] Backups de base de datos configurados
- [ ] Documentaci√≥n le√≠da y comprendida

---

**FIN DOCUMENTO - SISTEMA LISTO PARA PRODUCCI√ìN** ‚úÖ

**Pr√≥ximos Pasos Recomendados:**
1. Ejecutar scraping completo de validaci√≥n
2. Configurar scheduler semanal (lunes y jueves)
3. Monitorear primeras 2-3 ejecuciones autom√°ticas
4. Implementar dashboard de m√©tricas (opcional)
5. Proceder con parseo ESCO y vinculaci√≥n sem√°ntica
