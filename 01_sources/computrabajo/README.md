# ComputRabajo Scraper

## ğŸ¯ DescripciÃ³n

Scraper completo para ar.computrabajo.com que extrae ofertas laborales usando HTML scraping (requests + BeautifulSoup).

## ğŸ“Š Datos Clave

- **Ofertas disponibles**: ~500-1,000+ por keyword
- **MetodologÃ­a**: HTML Scraping (requests + BeautifulSoup)
- **Campos extraÃ­dos**: 13 por oferta
- **Formato**: CSV, JSON, Excel

---

## âš ï¸ IMPORTANTE: Requiere Diccionario de BÃºsqueda

**ComputRabajo NO devuelve ofertas en la home page sin bÃºsqueda especÃ­fica.**

Para scrapear, debes usar:
1. **Script simple** (`computrabajo_scraper.py`) - 1 keyword manual
2. **Script multi-keyword** (`scrapear_con_diccionario.py`) - MÃºltiples keywords automÃ¡tico âœ… Recomendado

---

## ğŸ“ Estructura

```
computrabajo/
â”œâ”€â”€ scrapers/
â”‚   â”œâ”€â”€ computrabajo_scraper.py            # Scraper base (1 keyword)
â”‚   â”œâ”€â”€ scrapear_con_diccionario.py        # Multi-keyword âœ… Usar este
â”‚   â”œâ”€â”€ computrabajo_explorer.py           # Explorador tÃ©cnico
â”‚   â”œâ”€â”€ test_requests.py                   # Test de metodologÃ­a
â”‚   â””â”€â”€ analizar_html.py                   # AnÃ¡lisis de estructura
â”œâ”€â”€ config/
â”‚   â””â”€â”€ search_keywords.json               # Diccionario de bÃºsquedas
â”œâ”€â”€ data/
â”‚   â””â”€â”€ raw/                                # Datos scrapeados
â””â”€â”€ README.md                               # Este archivo
```

---

## ğŸš€ Uso RÃ¡pido

### InstalaciÃ³n

```bash
cd 01_sources/computrabajo
pip install requests pandas beautifulsoup4 openpyxl
```

### MÃ©todo Recomendado: Multi-Keyword

```bash
cd scrapers
python scrapear_con_diccionario.py
```

Esto scrapearÃ¡ **4 keywords** (estrategia "minima") y consolidarÃ¡ resultados automÃ¡ticamente.

**Resultado esperado**: ~150-200 ofertas Ãºnicas

---

## ğŸ’» Uso ProgramÃ¡tico

### OpciÃ³n 1: Script Multi-Keyword (Recomendado)

```python
from scrapear_con_diccionario import ComputRabajoMultiSearch

# Crear scraper multi-keyword
scraper = ComputRabajoMultiSearch(
    delay_between_requests=2.0,
    delay_between_keywords=5.0
)

# Scrapear con estrategia predefinida
df_ofertas = scraper.scrapear_multiples_keywords(
    estrategia="general",  # 14 keywords
    max_paginas_por_keyword=5,
    max_resultados_por_keyword=100
)

# Guardar resultados
files = scraper.guardar_resultados(df_ofertas)
```

### OpciÃ³n 2: Keywords Personalizadas

```python
# Keywords custom
mis_keywords = ["python", "react", "data-analyst", "contabilidad"]

df_ofertas = scraper.scrapear_multiples_keywords(
    keywords=mis_keywords,
    max_paginas_por_keyword=3
)
```

### OpciÃ³n 3: Scraper Simple (1 keyword)

```python
from computrabajo_scraper import ComputRabajoScraper

scraper = ComputRabajoScraper(delay_between_requests=2.0)

# Solo 1 keyword
ofertas = scraper.scrapear_todo(
    max_paginas=5,
    query="python"  # Requerido!
)

scraper.save_to_csv(ofertas)
```

---

## ğŸ“– Diccionario de BÃºsquedas

UbicaciÃ³n: `config/search_keywords.json`

### Estrategias Disponibles

| Estrategia | Keywords | DescripciÃ³n | Uso |
|---|---|---|---|
| **minima** | 4 | Testing rÃ¡pido | Development |
| **general** | 14 | Ãreas principales | ProducciÃ³n âœ… |
| **tecnologia** | 20 | IT/Tech focus | AnÃ¡lisis IT |
| **amplia** | 15 | MÃ¡xima cobertura | Data collection |

### Editar Diccionario

Archivo: `config/search_keywords.json`

```json
{
  "estrategias": {
    "mi_estrategia": {
      "descripcion": "Mi estrategia custom",
      "keywords": [
        "python",
        "java",
        "ventas"
      ]
    }
  }
}
```

---

## ğŸ“Š Campos Disponibles

### InformaciÃ³n BÃ¡sica
- `id_oferta`: ID Ãºnico de la oferta
- `titulo`: TÃ­tulo del puesto
- `empresa`: Nombre de la empresa
- `empresa_url`: URL de la empresa
- `empresa_rating`: Rating (1.0-5.0)

### UbicaciÃ³n
- `ubicacion`: "Barrio/Ciudad, Capital Federal"
- Parseado a: `ciudad` y `provincia` en normalizaciÃ³n

### Modalidad
- `modalidad`: Remoto/Presencial/HÃ­brido

### Fechas
- `fecha_publicacion`: ISO 8601
- `fecha_publicacion_raw`: "Hace X horas", "Ayer"

### URLs
- `url_completa`: URL de la oferta
- `url_relativa`: Path relativo

### Metadata
- `scrapeado_en`: Timestamp ISO
- `fuente`: "computrabajo"
- `keyword_busqueda`: Keyword usada (solo multi-keyword)

**Total**: 13-14 campos

---

## âš™ï¸ ConfiguraciÃ³n

### ParÃ¡metros del Scraper

```python
# Scraper base
scraper = ComputRabajoScraper(
    delay_between_requests=2.0  # Segundos entre pÃ¡ginas
)

# Multi-keyword
multi_scraper = ComputRabajoMultiSearch(
    delay_between_requests=2.0,  # Entre pÃ¡ginas
    delay_between_keywords=5.0   # Entre keywords
)

# Scraping
df = scraper.scrapear_multiples_keywords(
    estrategia="general",           # Estrategia a usar
    max_paginas_por_keyword=5,      # PÃ¡ginas por keyword
    max_resultados_por_keyword=100  # Ofertas por keyword
)
```

### Rate Limiting

**Recomendado**:
- Entre pÃ¡ginas: 2 segundos
- Entre keywords: 5 segundos

**No bajar de**:
- Entre pÃ¡ginas: 1 segundo
- Entre keywords: 3 segundos

---

## ğŸ“ˆ Performance

- **Velocidad**: ~20 ofertas/minuto (con delays recomendados)
- **Recursos**: CPU < 5%, RAM ~30 MB
- **Almacenamiento**: ~2-3 KB por oferta
- **DeduplicaciÃ³n**: ~1-2% duplicados entre keywords

### Tiempo Estimado por Estrategia

| Estrategia | Keywords | Ofertas (aprox) | Tiempo |
|---|---|---|---|
| minima | 4 | 150-200 | ~1-2 min |
| general | 14 | 500-700 | ~5-7 min |
| tecnologia | 20 | 700-1000 | ~8-12 min |
| amplia | 15 | 600-800 | ~6-9 min |

---

## ğŸ” MetodologÃ­a TÃ©cnica

### Â¿Por quÃ© HTML Scraping y no API?

**InvestigaciÃ³n realizada**:
1. âœ… Playwright - InterceptÃ³ network calls
2. âœ… NO se encontrÃ³ API REST pÃºblica
3. âœ… Ofertas estÃ¡n en HTML Server-Side Rendered
4. âœ… `requests` funciona sin JavaScript rendering

**Ventajas**:
- RÃ¡pido (sin navegador)
- Bajo consumo de recursos
- Simple de mantener
- Efectivo (20 ofertas/pÃ¡gina)

### Estructura HTML

```html
<article class="box_offer" data-id="2E9C1804DA7CAC8F...">
  <h2>
    <a class="js-o-link">Senior Python Developer</a>
  </h2>
  <p class="dFlex">
    <a href="/empresa">Kaizen Recursos Humanos</a>
    <span class="fwB">4.1</span>
  </p>
  <p class="fs16">
    <span class="mr10">Monserrat, Capital Federal</span>
  </p>
  <div class="fs13">
    <span class="icon i_home">Remoto</span>
  </div>
  <p class="fc_aux">Hace 4 horas</p>
</article>
```

---

## ğŸ”„ IntegraciÃ³n con Pipeline

### ConsolidaciÃ³n

Las ofertas de ComputRabajo se normalizan automÃ¡ticamente al schema unificado:

```bash
cd ../../02_consolidation/scripts
python consolidar_fuentes.py --fuentes computrabajo
```

**Mapeo de campos**:
- `id_oferta` â†’ `_metadata.source_id`
- `titulo` â†’ `informacion_basica.titulo`
- `ubicacion` â†’ `ubicacion.ubicacion_raw` (parseado a ciudad/provincia)
- `modalidad` â†’ `modalidad.modalidad_trabajo` (normalizado)
- `empresa_rating` â†’ `source_specific.empresa_rating`

### Pipeline Completo

```bash
cd ../..
python pipeline_completo.py --all --fuentes computrabajo
```

Ejecuta:
1. Scraping (con diccionario)
2. NormalizaciÃ³n
3. ESCO Matching
4. AnÃ¡lisis
5. Productos finales

---

## ğŸ§ª Tests

### Test de MetodologÃ­a

```bash
cd scrapers
python test_requests.py
```

Verifica que requests funciona (vs Playwright).

### Test del Scraper Simple

```bash
python computrabajo_scraper.py
```

Scrapea ofertas de "python" como ejemplo.

### Test Multi-Keyword

```bash
python scrapear_con_diccionario.py
```

Scrapea 4 keywords y consolida resultados.

---

## âš–ï¸ Consideraciones Legales

### âœ… Antes de Usar

1. Lee los [TÃ©rminos de Servicio](https://ar.computrabajo.com/terminos-y-condiciones)
2. Verifica [robots.txt](https://ar.computrabajo.com/robots.txt)
3. Implementa rate limiting (mÃ­nimo 2 segundos)
4. Usa User-Agent identificable
5. Solo para investigaciÃ³n/anÃ¡lisis personal

### LÃ­mites Recomendados

- **Delay mÃ­nimo entre pÃ¡ginas**: 2 segundos
- **Delay mÃ­nimo entre keywords**: 5 segundos
- **Ofertas por sesiÃ³n**: Hasta 500-1,000
- **Frecuencia**: No mÃ¡s de 1 vez por hora

---

## ğŸ› Troubleshooting

### No se obtienen ofertas

**Problema**: El scraper devuelve 0 ofertas.

**SoluciÃ³n**:
- âœ… Usar `scrapear_con_diccionario.py` (multi-keyword)
- âœ… O especificar `query=` en scraper simple
- âŒ NO funciona sin keyword

### Muchos duplicados

**Problema**: Mismas ofertas en mÃºltiples keywords.

**SoluciÃ³n**: El script multi-keyword deduplica automÃ¡ticamente por `id_oferta`.

### UbicaciÃ³n mal parseada

**Problema**: Ciudad/provincia incorrectos.

**SoluciÃ³n**: Ya corregido en normalizer. Formato esperado: "Ciudad, Capital Federal" â†’ Ciudad="Ciudad", Provincia="Buenos Aires"

---

## ğŸ“ Ejemplos de Uso

### Caso 1: AnÃ¡lisis del Mercado IT

```python
from scrapear_con_diccionario import ComputRabajoMultiSearch

scraper = ComputRabajoMultiSearch()

# Usar estrategia "tecnologia" (20 keywords IT)
df = scraper.scrapear_multiples_keywords(
    estrategia="tecnologia",
    max_paginas_por_keyword=10
)

# AnÃ¡lisis
print(df['keyword_busqueda'].value_counts())
print(df.groupby('modalidad')['id_oferta'].count())

scraper.guardar_resultados(df, "analisis_it")
```

### Caso 2: Ofertas Remotas

```python
# Scrapear
df = scraper.scrapear_multiples_keywords(estrategia="general")

# Filtrar remotas
remotas = df[df['modalidad'] == 'Remoto']

print(f"Ofertas remotas: {len(remotas)}/{len(df)}")
```

### Caso 3: Por Provincia

```python
from normalizar_campos import ComputRabajoNormalizer

# Scrapear
df_raw = scraper.scrapear_multiples_keywords(estrategia="amplia")

# Normalizar
normalizer = ComputRabajoNormalizer()
df_norm = normalizer.normalize(df_raw)

# Agrupar por provincia
por_provincia = df_norm.groupby('ubicacion.provincia').size()
print(por_provincia.sort_values(ascending=False))
```

---

## ğŸ“š DocumentaciÃ³n Adicional

- [Schema Unificado](../../shared/schemas/SCHEMA_DOCUMENTATION.md)
- [Pipeline Completo](../../docs/arquitectura.md)
- [Comparativa de Fuentes](../../RESUMEN_BUMERAN.md)

---

## ğŸ”— Enlaces Ãštiles

- [ComputRabajo Argentina](https://ar.computrabajo.com)
- [TÃ©rminos de Servicio](https://ar.computrabajo.com/terminos-y-condiciones)
- [robots.txt](https://ar.computrabajo.com/robots.txt)

---

## ğŸ“Š EstadÃ­sticas

### Datos de ImplementaciÃ³n (2025-10-21)

- **Ofertas disponibles**: ~500-1,000+ por keyword
- **Ofertas scrapeadas (test)**: 158 (4 keywords)
- **Campos por oferta**: 13-14
- **Formatos de exportaciÃ³n**: CSV, JSON, Excel
- **Tiempo de implementaciÃ³n**: 2-3 horas
- **Duplicados promedio**: 1-2%

### Comparativa vs Otras Fuentes

| MÃ©trica | ZonaJobs | Bumeran | **ComputRabajo** |
|---|---|---|---|
| Ofertas | ~3,000 | ~12,000 | **~1,000/keyword** |
| MetodologÃ­a | API REST | API REST | **HTML Scraping** |
| Complejidad | Media | Media | **Media** |
| Campos | 33 | 32 | **13** |
| **Requiere keywords** | âŒ | âŒ | **âœ… SÃ­** |

---

## ğŸ¯ Tips y Recomendaciones

1. **Siempre usa el script multi-keyword** para maximizar cobertura
2. **Estrategia "general"** es ideal para producciÃ³n (14 keywords)
3. **DeduplicaciÃ³n automÃ¡tica** elimina ~1-2% duplicados
4. **Respeta rate limiting** (2s mÃ­nimo) para evitar bloqueos
5. **Customiza keywords** segÃºn tu necesidad especÃ­fica
6. **UbicaciÃ³n parseada** automÃ¡ticamente en normalizaciÃ³n

---

**Scraper implementado**: 2025-10-21
**VersiÃ³n**: 1.0
**Estado**: âœ… Funcional (requiere diccionario)
**Mantenedor**: OEDE
