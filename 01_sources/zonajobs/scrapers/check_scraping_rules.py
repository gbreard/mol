"""
Script para verificar reglas de scraping antes de comenzar
Verifica robots.txt y ayuda a identificar restricciones
"""

import requests
from urllib.parse import urlparse, urljoin
from urllib.robotparser import RobotFileParser
import time


class ScrapingRulesChecker:
    """Verifica si el scraping est√° permitido en un sitio"""

    def __init__(self, base_url: str):
        """
        Args:
            base_url: URL base del sitio (ej: https://www.zonajobs.com.ar)
        """
        self.base_url = base_url
        self.domain = urlparse(base_url).netloc
        self.robots_url = urljoin(base_url, '/robots.txt')
        self.robot_parser = RobotFileParser()

    def check_robots_txt(self):
        """Verifica y muestra el contenido de robots.txt"""
        print("=" * 80)
        print("ü§ñ VERIFICANDO ROBOTS.TXT")
        print("=" * 80)
        print(f"URL: {self.robots_url}\n")

        try:
            response = requests.get(self.robots_url, timeout=10)

            if response.status_code == 200:
                print("‚úì robots.txt encontrado\n")
                print("üìÑ CONTENIDO:")
                print("-" * 80)
                print(response.text)
                print("-" * 80)

                # Parsear robots.txt
                self.robot_parser.set_url(self.robots_url)
                self.robot_parser.read()

                return True
            else:
                print(f"‚ö†Ô∏è  robots.txt no encontrado (Status: {response.status_code})")
                print("üí° Esto puede significar que no hay restricciones espec√≠ficas")
                return False

        except Exception as e:
            print(f"‚ùå Error al obtener robots.txt: {e}")
            return False

    def can_fetch(self, url: str, user_agent: str = "*") -> bool:
        """
        Verifica si una URL espec√≠fica puede ser scrapeada

        Args:
            url: URL a verificar
            user_agent: User agent a usar

        Returns:
            True si se puede scrapear, False si no
        """
        if not self.robot_parser.url:
            self.check_robots_txt()

        can_fetch = self.robot_parser.can_fetch(user_agent, url)

        print(f"\nüîç Verificando: {url}")
        print(f"   User-Agent: {user_agent}")
        print(f"   {'‚úÖ PERMITIDO' if can_fetch else '‚ùå NO PERMITIDO'}")

        return can_fetch

    def get_crawl_delay(self, user_agent: str = "*") -> float:
        """
        Obtiene el crawl delay recomendado

        Args:
            user_agent: User agent

        Returns:
            Delay en segundos, o 0 si no est√° especificado
        """
        if not self.robot_parser.url:
            self.check_robots_txt()

        delay = self.robot_parser.crawl_delay(user_agent)

        print(f"\n‚è±Ô∏è  Crawl Delay para '{user_agent}':")
        if delay:
            print(f"   {delay} segundos (DEBE RESPETARSE)")
        else:
            print(f"   No especificado (se recomienda 1-2 segundos)")

        return delay or 1.0

    def check_common_paths(self):
        """Verifica si paths comunes est√°n permitidos"""
        print("\n" + "=" * 80)
        print("üõ£Ô∏è  VERIFICANDO PATHS COMUNES")
        print("=" * 80)

        common_paths = [
            '/empleos',
            '/api/',
            '/api/jobs',
            '/api/search',
            '/buscar',
            '/ofertas',
        ]

        results = []

        for path in common_paths:
            url = urljoin(self.base_url, path)
            allowed = self.can_fetch(url)
            results.append((path, allowed))
            time.sleep(0.5)

        # Resumen
        print("\nüìä RESUMEN:")
        print("-" * 80)
        for path, allowed in results:
            status = "‚úÖ PERMITIDO" if allowed else "‚ùå BLOQUEADO"
            print(f"   {path:<30} {status}")

        return results

    def check_terms_of_service(self):
        """Muestra informaci√≥n sobre t√©rminos de servicio"""
        print("\n" + "=" * 80)
        print("üìú T√âRMINOS DE SERVICIO")
        print("=" * 80)

        common_tos_urls = [
            '/terminos-y-condiciones',
            '/terms',
            '/terminos',
            '/legal/terms',
        ]

        print("\nüí° Verifica manualmente los t√©rminos de servicio en:")

        for path in common_tos_urls:
            url = urljoin(self.base_url, path)
            print(f"   ‚Ä¢ {url}")

            try:
                response = requests.head(url, timeout=5)
                if response.status_code == 200:
                    print(f"     ‚úì Encontrado: {url}")
                    break
            except:
                pass

        print("\n‚ö†Ô∏è  PUNTOS IMPORTANTES A VERIFICAR:")
        print("   1. ¬øEst√° expl√≠citamente prohibido el scraping/crawling?")
        print("   2. ¬øHay restricciones sobre uso automatizado?")
        print("   3. ¬øQu√© datos est√°n permitidos extraer?")
        print("   4. ¬øHay l√≠mites de rate (requests por segundo/minuto)?")
        print("   5. ¬øSe requiere atribuci√≥n o permiso previo?")

    def generate_recommendations(self):
        """Genera recomendaciones de scraping √©tico"""
        print("\n" + "=" * 80)
        print("‚úÖ RECOMENDACIONES DE SCRAPING √âTICO")
        print("=" * 80)

        delay = self.get_crawl_delay()

        recommendations = f"""
1. üïê RATE LIMITING:
   ‚Ä¢ Implementar delay de al menos {delay} segundos entre requests
   ‚Ä¢ Considerar delays m√°s largos durante horas pico
   ‚Ä¢ No hacer m√°s de 1 request por segundo

2. üé≠ IDENTIFICACI√ìN:
   ‚Ä¢ Usar un User-Agent descriptivo e identificable
   ‚Ä¢ Ejemplo: "MiBot/1.0 (Investigaci√≥n acad√©mica; contacto@email.com)"
   ‚Ä¢ NO intentar ocultar que eres un bot

3. üìä VOLUMEN DE DATOS:
   ‚Ä¢ No descargar TODO el sitio
   ‚Ä¢ Limitar a datos necesarios
   ‚Ä¢ Implementar cach√© para evitar re-scraping

4. ‚è∞ HORARIOS:
   ‚Ä¢ Preferir horarios de baja actividad
   ‚Ä¢ Evitar horarios pico (9-18hs en d√≠as laborales)

5. üîÑ MANEJO DE ERRORES:
   ‚Ä¢ Respetar c√≥digos HTTP 429 (Too Many Requests)
   ‚Ä¢ Implementar exponential backoff en errores
   ‚Ä¢ No reintentar agresivamente

6. üíæ USO DE DATOS:
   ‚Ä¢ No redistribuir p√∫blicamente sin permiso
   ‚Ä¢ Respetar datos personales (GDPR, Ley de Protecci√≥n de Datos)
   ‚Ä¢ Usar solo para prop√≥sitos leg√≠timos declarados

7. üîí SEGURIDAD:
   ‚Ä¢ No intentar bypassear medidas de seguridad
   ‚Ä¢ No intentar acceder a √°reas protegidas
   ‚Ä¢ Reportar vulnerabilidades de forma responsable

8. üìû COMUNICACI√ìN:
   ‚Ä¢ Considerar contactar al sitio antes de scrapear masivamente
   ‚Ä¢ Preguntar si tienen una API oficial
   ‚Ä¢ Estar abierto a feedback del sitio
        """

        print(recommendations)

    def full_check(self):
        """Ejecuta todas las verificaciones"""
        print("\n" + "=" * 80)
        print(f"üîç AN√ÅLISIS COMPLETO DE REGLAS DE SCRAPING")
        print(f"Sitio: {self.base_url}")
        print("=" * 80 + "\n")

        # 1. Verificar robots.txt
        self.check_robots_txt()

        # 2. Verificar paths comunes
        self.check_common_paths()

        # 3. Informaci√≥n sobre t√©rminos
        self.check_terms_of_service()

        # 4. Recomendaciones
        self.generate_recommendations()

        print("\n" + "=" * 80)
        print("‚úÖ VERIFICACI√ìN COMPLETADA")
        print("=" * 80)


def main():
    """Funci√≥n principal"""

    # ZonaJobs
    checker = ScrapingRulesChecker("https://www.zonajobs.com.ar")
    checker.full_check()

    print("\n\nüí° PR√ìXIMOS PASOS:")
    print("   1. Lee cuidadosamente los t√©rminos de servicio")
    print("   2. Verifica que tu uso est√© permitido")
    print("   3. Implementa las recomendaciones en tu scraper")
    print("   4. Mant√©n un registro de tus actividades de scraping")
    print("   5. S√© transparente sobre tu identidad y prop√≥sito")


if __name__ == "__main__":
    main()
