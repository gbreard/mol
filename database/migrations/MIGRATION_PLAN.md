# PLAN INTEGRADO DE MIGRACI√ìN - Database v2.0
## Sistema de Inteligencia del Mercado Laboral Argentino

**Versi√≥n**: 2.0.0
**Fecha de creaci√≥n**: 2025-11-02
**Estado**: En preparaci√≥n
**Fuente primaria**: Bumeran (expandible a otras fuentes en futuro)

---

## TABLA DE CONTENIDOS

1. [Visi√≥n General del Sistema](#1-visi√≥n-general-del-sistema)
2. [Estado Actual y Problemas Identificados](#2-estado-actual-y-problemas-identificados)
3. [Database v2.0 - Arquitectura Completa](#3-database-v20---arquitectura-completa)
4. [Estrategia de Migraci√≥n](#4-estrategia-de-migraci√≥n)
5. [Pipeline End-to-End Bumeran](#5-pipeline-end-to-end-bumeran)
6. [Fixes Cr√≠ticos](#6-fixes-cr√≠ticos)
7. [Timeline de Implementaci√≥n](#7-timeline-de-implementaci√≥n)
8. [Dashboard Integration](#8-dashboard-integration)
9. [Criterios de Validaci√≥n](#9-criterios-de-validaci√≥n)
10. [Rollback Plan](#10-rollback-plan)

---

## 1. VISI√ìN GENERAL DEL SISTEMA

El sistema es una **plataforma de aprendizaje multi-capa** para an√°lisis del mercado laboral argentino:

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                    CAPA 1: SCRAPING                             ‚îÇ
‚îÇ  Bumeran ‚Üí Keywords ‚Üí Incremental tracking ‚Üí Raw offers         ‚îÇ
‚îÇ  Scheduler: Lunes y Jueves 8:00 AM (PRODUCCI√ìN - NO ROMPER)    ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                              ‚Üì
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                    CAPA 2: NLP EXTRACTION                        ‚îÇ
‚îÇ  Regex v3.7 (70-80%) + LLM v4.0 (95%+) ‚Üí Structured data        ‚îÇ
‚îÇ  Extrae: experiencia, educaci√≥n, skills, salario, etc.          ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                              ‚Üì
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                    CAPA 3: ESCO MATCHING                         ‚îÇ
‚îÇ  RDF Ontology (17 tablas) ‚Üí Semantic enrichment                 ‚îÇ
‚îÇ  Occupations + Skills + BGE-M3 embeddings                        ‚îÇ
‚îÇ  BUG CR√çTICO: 6 skills en DB (esperado: 13,890)                 ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                              ‚Üì
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                    CAPA 4: VISUALIZACI√ìN                         ‚îÇ
‚îÇ  ‚Ä¢ Dashboard Operativo (Dash) - localhost - Monitoreo calidad   ‚îÇ
‚îÇ  ‚Ä¢ Dashboard P√∫blico (Shiny) - shinyapps.io - Analytics         ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

### Objetivo de la Migraci√≥n

Dise√±ar una base de datos relacional v2.0 que:
- **Soporte el pipeline completo** sin necesidad de parches constantes
- **Preserve la producci√≥n** (scheduler Monday/Thursday 8:00 AM sigue funcionando)
- **Habilite versioning** de NLP (comparar regex vs LLM)
- **Normalice datos** (skills en tablas relacionales, no JSON)
- **Integre ESCO** correctamente (fix bug de 6 ‚Üí 13,890 skills)
- **Permita an√°lisis temporal** (tracking de cambios en ofertas)
- **Sea visible** en dashboard operativo en tiempo real

---

## 2. ESTADO ACTUAL Y PROBLEMAS IDENTIFICADOS

### 2.1 Base de Datos Actual (v1.0)

**Archivo**: `D:/OEDE/Webscrapping/database/bumeran_scraping.db`
**Tama√±o**: ~30 MB
**Total ofertas**: 5,479
**Total tablas**: 22

#### Tablas Core v1.0

```
ofertas (38 columnas)
‚îú‚îÄ‚îÄ id_oferta (PK, TEXT)
‚îú‚îÄ‚îÄ titulo, descripcion, empresa
‚îú‚îÄ‚îÄ fecha_publicacion (3 formatos distintos!)
‚îú‚îÄ‚îÄ provincia, localidad
‚îú‚îÄ‚îÄ area_trabajo
‚îî‚îÄ‚îÄ ... (35 campos m√°s)

ofertas_nlp
‚îú‚îÄ‚îÄ id_oferta (TEXT, sin FK formal)
‚îú‚îÄ‚îÄ experiencia_min_anios, experiencia_max_anios
‚îú‚îÄ‚îÄ nivel_educativo, estado_educativo
‚îú‚îÄ‚îÄ skills_tecnicas_list (JSON string)  ‚Üê PROBLEMA
‚îú‚îÄ‚îÄ soft_skills_list (JSON string)      ‚Üê PROBLEMA
‚îú‚îÄ‚îÄ nlp_version (TEXT, sobrescribe)     ‚Üê PROBLEMA
‚îî‚îÄ‚îÄ nlp_confidence_score

ofertas_esco_matching
‚îú‚îÄ‚îÄ oferta_id (TEXT, sin FK formal)
‚îú‚îÄ‚îÄ matched_occupation_uri
‚îú‚îÄ‚îÄ occupation_confidence
‚îú‚îÄ‚îÄ matched_skills_uris (JSON)          ‚Üê PROBLEMA
‚îî‚îÄ‚îÄ skills_confidence
```

#### 17 Tablas ESCO (RDF Ontology)

```
# Occupations
esco_occupations                    (Conceptos de ocupaciones)
esco_occupation_labels_es           (Labels en espa√±ol)
esco_occupation_descriptions        (Descripciones)

# Skills
esco_skills                         (BUG: solo 6 registros, deber√≠a tener ~13,890)
esco_skill_labels_es
esco_skill_descriptions

# Relaciones sem√°nticas
esco_occupation_essential_skills    (N:M occupation ‚Üî essential skills)
esco_occupation_optional_skills     (N:M occupation ‚Üî optional skills)
esco_occupation_broader             (Jerarqu√≠a: broader concepts)
esco_occupation_narrower            (Jerarqu√≠a: narrower concepts)
esco_skill_broader
esco_skill_narrower

# Localizaci√≥n argentina
diccionario_arg_esco_occupations    (T√≠tulos AR ‚Üí ESCO)
diccionario_arg_esco_skills         (Skills AR ‚Üí ESCO)

# An√°lisis
ofertas_esco_matching               (Ofertas ‚Üí ESCO matches)
esco_gap_analysis                   (Skills en ofertas pero no en ESCO)
```

### 2.2 Problemas Cr√≠ticos Identificados

#### ‚ùå PROBLEMA 1: Skills en JSON (No Queryable)

```sql
-- IMPOSIBLE hacer esta query:
SELECT COUNT(*) FROM ofertas_nlp WHERE skills_tecnicas_list LIKE '%Python%';

-- Raz√≥n: skills_tecnicas_list = '["Python", "Django", "PostgreSQL"]' (JSON string)
```

**Impacto**: No se puede analizar "¬øCu√°ntas ofertas requieren Python?" sin parsear JSON en app.

#### ‚ùå PROBLEMA 2: NLP Version Sobrescribe Datos

```sql
-- Si re-ejecutamos NLP con nueva versi√≥n:
UPDATE ofertas_nlp SET nlp_version = 'v4.0', skills_tecnicas_list = '...' WHERE id_oferta = '123';

-- PERDEMOS la versi√≥n anterior (v3.7) y sus resultados
```

**Impacto**: No podemos comparar "¬øLLM v4.0 extrajo m√°s skills que Regex v3.7?"

#### ‚ùå PROBLEMA 3: Sin Tracking de Cambios

```sql
-- Si una oferta cambia de t√≠tulo o sueldo:
INSERT OR REPLACE INTO ofertas VALUES (...);

-- PERDEMOS el valor anterior, no hay historial
```

**Impacto**: No podemos analizar "¬øCu√°ntas ofertas aumentaron el salario en los √∫ltimos 30 d√≠as?"

#### ‚ùå PROBLEMA 4: Redundancia de Fechas (9 columnas!)

```
fecha_publicacion      (TEXT ISO)
fecha_publicacion_ts   (INTEGER timestamp)
fecha_publicacion_dt   (TEXT datetime format)
fecha_scraping         (TEXT ISO)
fecha_scraping_ts      (INTEGER)
fecha_scraping_dt      (TEXT)
fecha_actualizacion    (TEXT)
fecha_actualizacion_ts (INTEGER)
fecha_actualizacion_dt (TEXT)
```

**Impacto**: Confusi√≥n, desperdicio de espacio, 3 formatos para el mismo dato.

#### ‚ùå PROBLEMA 5: ESCO Skills Bug CR√çTICO

```sql
SELECT COUNT(*) FROM esco_skills;
-- Resultado: 6 (deber√≠a ser ~13,890)
```

**Causa**: Bug en `populate_esco_from_rdf.py` l√≠neas 200-300 (SPARQL query incompleto)
**Impacto**: ESCO matching completamente in√∫til (no hay skills para matchear)

#### ‚ùå PROBLEMA 6: Sin Foreign Keys Formales

```sql
-- No hay CASCADE, no hay validaci√≥n referencial
id_oferta en ofertas_nlp puede referenciar oferta inexistente
```

**Impacto**: Posibles registros hu√©rfanos, inconsistencias.

---

## 3. DATABASE v2.0 - ARQUITECTURA COMPLETA

### 3.1 Principios de Dise√±o

1. **Normalizaci√≥n**: Skills, keywords, empresas en tablas separadas con N:M
2. **Versionado**: M√∫ltiples versiones NLP por oferta (comparaci√≥n A/B)
3. **Auditor√≠a**: Tracking completo de cambios con ofertas_historial
4. **Integridad**: Foreign keys con CASCADE expl√≠citos
5. **Performance**: √çndices en campos de b√∫squeda frecuente
6. **ESCO-first**: Integraci√≥n nativa con ontolog√≠a RDF
7. **Backward-compatible**: v1 schema se mantiene para scheduler

### 3.2 Esquema Completo v2.0

#### CAPA 1: SCRAPING (4 tablas nuevas)

```sql
-- =====================================================================
-- TABLA: scraping_sessions
-- Tracking de cada ejecuci√≥n del scraper
-- =====================================================================
CREATE TABLE scraping_sessions (
    id INTEGER PRIMARY KEY AUTOINCREMENT,
    session_uuid TEXT UNIQUE NOT NULL,
    source TEXT NOT NULL,              -- 'bumeran', 'indeed' (futuro)
    mode TEXT NOT NULL,                -- 'full', 'incremental'
    start_time TEXT NOT NULL,
    end_time TEXT,
    ofertas_total INTEGER DEFAULT 0,
    ofertas_nuevas INTEGER DEFAULT 0,
    ofertas_actualizadas INTEGER DEFAULT 0,
    keywords_used TEXT,                -- JSON array de keywords usadas
    status TEXT DEFAULT 'running',     -- 'running', 'completed', 'failed'
    error_message TEXT,
    created_at TEXT DEFAULT (datetime('now'))
);

CREATE INDEX idx_sessions_source ON scraping_sessions(source);
CREATE INDEX idx_sessions_start ON scraping_sessions(start_time);

-- =====================================================================
-- TABLA: ofertas_raw
-- Inmutable audit log - NUNCA se modifica, solo INSERT
-- =====================================================================
CREATE TABLE ofertas_raw (
    id INTEGER PRIMARY KEY AUTOINCREMENT,
    id_oferta TEXT NOT NULL,
    scraping_session_id INTEGER NOT NULL,
    raw_json TEXT NOT NULL,            -- JSON completo de la oferta
    content_hash TEXT,                 -- SHA256 para detectar cambios
    scraped_at TEXT DEFAULT (datetime('now')),
    FOREIGN KEY (scraping_session_id) REFERENCES scraping_sessions(id) ON DELETE CASCADE
);

CREATE INDEX idx_raw_oferta ON ofertas_raw(id_oferta);
CREATE INDEX idx_raw_session ON ofertas_raw(scraping_session_id);
CREATE INDEX idx_raw_hash ON ofertas_raw(content_hash);

-- =====================================================================
-- TABLA: ofertas_v2 (normalized)
-- Tabla principal normalizada, reemplaza 38 columnas de ofertas v1
-- =====================================================================
CREATE TABLE ofertas_v2 (
    id INTEGER PRIMARY KEY AUTOINCREMENT,
    id_oferta TEXT UNIQUE NOT NULL,    -- ID de Bumeran
    titulo TEXT NOT NULL,
    descripcion TEXT,
    empresa_id INTEGER,                -- FK a empresas (nueva tabla)
    url TEXT,
    provincia TEXT,
    localidad TEXT,
    area_trabajo TEXT,
    tipo_contrato TEXT,                -- 'full-time', 'part-time', etc.
    modalidad_trabajo TEXT,            -- 'presencial', 'remoto', 'h√≠brido'

    -- Fechas (solo 3, no 9!)
    fecha_publicacion TEXT,
    fecha_actualizacion TEXT,
    primera_vez_scrapeada TEXT,

    -- Estado
    is_active INTEGER DEFAULT 1,       -- 0 = oferta expir√≥

    -- Metadata
    source TEXT DEFAULT 'bumeran',
    created_at TEXT DEFAULT (datetime('now')),
    updated_at TEXT DEFAULT (datetime('now')),

    FOREIGN KEY (empresa_id) REFERENCES empresas(id) ON DELETE SET NULL
);

CREATE INDEX idx_ofertas_v2_id_oferta ON ofertas_v2(id_oferta);
CREATE INDEX idx_ofertas_v2_titulo ON ofertas_v2(titulo);
CREATE INDEX idx_ofertas_v2_empresa ON ofertas_v2(empresa_id);
CREATE INDEX idx_ofertas_v2_provincia ON ofertas_v2(provincia);
CREATE INDEX idx_ofertas_v2_fecha_pub ON ofertas_v2(fecha_publicacion);

-- =====================================================================
-- TABLA: empresas (nueva)
-- Normalizaci√≥n de empresas
-- =====================================================================
CREATE TABLE empresas (
    id INTEGER PRIMARY KEY AUTOINCREMENT,
    nombre TEXT UNIQUE NOT NULL,
    sector TEXT,                       -- 'Tecnolog√≠a', 'Finanzas', etc.
    tamanio TEXT,                      -- 'startup', 'pyme', 'grande'
    ofertas_count INTEGER DEFAULT 0,   -- Denormalizado para performance
    created_at TEXT DEFAULT (datetime('now'))
);

CREATE INDEX idx_empresas_nombre ON empresas(nombre);
```

#### CAPA 2: NLP EXTRACTION (5 tablas nuevas)

```sql
-- =====================================================================
-- TABLA: nlp_versions
-- Cat√°logo de versiones NLP (permite comparaci√≥n)
-- =====================================================================
CREATE TABLE nlp_versions (
    id INTEGER PRIMARY KEY AUTOINCREMENT,
    version_name TEXT UNIQUE NOT NULL,  -- 'v3.7-regex-aggressive'
    model_type TEXT NOT NULL,           -- 'regex', 'llm', 'hybrid'
    model_details TEXT,                 -- JSON con config del modelo
    is_active INTEGER DEFAULT 0,        -- Solo 1 activa a la vez
    avg_confidence_score REAL,          -- Calculado peri√≥dicamente
    created_at TEXT DEFAULT (datetime('now'))
);

INSERT INTO nlp_versions (version_name, model_type, is_active) VALUES
    ('v2.0-regex-basic', 'regex', 0),
    ('v3.7-regex-aggressive', 'regex', 0),
    ('v4.0-llm-llama3', 'llm', 0),
    ('v4.0-hybrid-regex-llm', 'hybrid', 1);

-- =====================================================================
-- TABLA: ofertas_nlp_v2
-- Versionado de extracciones NLP (m√∫ltiples versiones por oferta)
-- =====================================================================
CREATE TABLE ofertas_nlp_v2 (
    id INTEGER PRIMARY KEY AUTOINCREMENT,
    id_oferta TEXT NOT NULL,
    version_id INTEGER NOT NULL,

    -- Experiencia
    experiencia_min_anios INTEGER,
    experiencia_max_anios INTEGER,
    experiencia_area TEXT,

    -- Educaci√≥n
    nivel_educativo TEXT,              -- 'secundario', 'terciario', 'universitario', 'posgrado'
    estado_educativo TEXT,             -- 'completo', 'en_curso', 'incompleto'
    carrera_especifica TEXT,
    titulo_excluyente INTEGER,         -- Boolean: 1 = excluyente

    -- Idiomas
    idioma_principal TEXT,
    nivel_idioma_principal TEXT,
    idioma_secundario TEXT,
    nivel_idioma_secundario TEXT,

    -- Salario (extra√≠do de descripci√≥n)
    salario_min REAL,
    salario_max REAL,
    moneda TEXT,

    -- Jornada
    jornada_laboral TEXT,              -- 'full-time', 'part-time', 'freelance'
    horario_flexible INTEGER,          -- Boolean

    -- Metadata
    confidence_score REAL,             -- Score global de esta extracci√≥n
    extracted_at TEXT DEFAULT (datetime('now')),
    extraction_time_ms INTEGER,        -- Tiempo que tom√≥ el procesamiento

    UNIQUE(id_oferta, version_id),
    FOREIGN KEY (version_id) REFERENCES nlp_versions(id) ON DELETE CASCADE
);

CREATE INDEX idx_nlp_v2_oferta ON ofertas_nlp_v2(id_oferta);
CREATE INDEX idx_nlp_v2_version ON ofertas_nlp_v2(version_id);
CREATE INDEX idx_nlp_v2_confidence ON ofertas_nlp_v2(confidence_score);

-- =====================================================================
-- TABLA: skills (nueva - normalizaci√≥n)
-- Cat√°logo unificado de skills t√©cnicas
-- =====================================================================
CREATE TABLE skills (
    id INTEGER PRIMARY KEY AUTOINCREMENT,
    nombre TEXT UNIQUE NOT NULL,
    tipo TEXT NOT NULL,                -- 'tecnica', 'soft', 'certificacion'
    categoria TEXT,                    -- 'programaci√≥n', 'base de datos', etc.
    esco_skill_uri TEXT,               -- Link a ESCO (si existe)
    frecuencia_total INTEGER DEFAULT 0,-- Cu√°ntas ofertas la mencionan
    created_at TEXT DEFAULT (datetime('now'))
);

CREATE INDEX idx_skills_nombre ON skills(nombre);
CREATE INDEX idx_skills_tipo ON skills(tipo);
CREATE INDEX idx_skills_esco ON skills(esco_skill_uri);

-- =====================================================================
-- TABLA: ofertas_skills (N:M)
-- Relaci√≥n ofertas ‚Üî skills (VERSIONADA por NLP)
-- =====================================================================
CREATE TABLE ofertas_skills (
    id INTEGER PRIMARY KEY AUTOINCREMENT,
    oferta_id TEXT NOT NULL,
    skill_id INTEGER NOT NULL,
    version_id INTEGER NOT NULL,       -- Versi√≥n NLP que extrajo esta skill
    nivel TEXT,                        -- 'basico', 'intermedio', 'avanzado', 'excluyente'
    es_excluyente INTEGER DEFAULT 0,   -- Boolean

    UNIQUE(oferta_id, skill_id, version_id),
    FOREIGN KEY (skill_id) REFERENCES skills(id) ON DELETE CASCADE,
    FOREIGN KEY (version_id) REFERENCES nlp_versions(id) ON DELETE CASCADE
);

CREATE INDEX idx_ofertas_skills_oferta ON ofertas_skills(oferta_id);
CREATE INDEX idx_ofertas_skills_skill ON ofertas_skills(skill_id);
CREATE INDEX idx_ofertas_skills_version ON ofertas_skills(version_id);

-- =====================================================================
-- TABLA: soft_skills (nueva)
-- Cat√°logo de soft skills
-- =====================================================================
CREATE TABLE soft_skills (
    id INTEGER PRIMARY KEY AUTOINCREMENT,
    nombre TEXT UNIQUE NOT NULL,
    categoria TEXT,                    -- 'liderazgo', 'comunicaci√≥n', etc.
    frecuencia_total INTEGER DEFAULT 0,
    created_at TEXT DEFAULT (datetime('now'))
);

-- N:M entre ofertas y soft_skills (similar a ofertas_skills)
CREATE TABLE ofertas_soft_skills (
    id INTEGER PRIMARY KEY AUTOINCREMENT,
    oferta_id TEXT NOT NULL,
    soft_skill_id INTEGER NOT NULL,
    version_id INTEGER NOT NULL,

    UNIQUE(oferta_id, soft_skill_id, version_id),
    FOREIGN KEY (soft_skill_id) REFERENCES soft_skills(id) ON DELETE CASCADE,
    FOREIGN KEY (version_id) REFERENCES nlp_versions(id) ON DELETE CASCADE
);
```

#### CAPA 3: ESCO INTEGRATION (mantiene 17 tablas existentes + mejoras)

```sql
-- =====================================================================
-- Las 17 tablas ESCO existentes se MANTIENEN:
-- =====================================================================
-- esco_occupations
-- esco_occupation_labels_es
-- esco_occupation_descriptions
-- esco_skills                           ‚Üê FIX: 6 ‚Üí 13,890 registros
-- esco_skill_labels_es
-- esco_skill_descriptions
-- esco_occupation_essential_skills
-- esco_occupation_optional_skills
-- esco_occupation_broader
-- esco_occupation_narrower
-- esco_skill_broader
-- esco_skill_narrower
-- diccionario_arg_esco_occupations
-- diccionario_arg_esco_skills
-- ofertas_esco_matching
-- esco_gap_analysis

-- MEJORA: Agregar versioning a ofertas_esco_matching
ALTER TABLE ofertas_esco_matching ADD COLUMN matching_version TEXT DEFAULT 'v1.0';
ALTER TABLE ofertas_esco_matching ADD COLUMN matched_at TEXT DEFAULT (datetime('now'));

-- NUEVA TABLA: esco_matching_versions
CREATE TABLE esco_matching_versions (
    id INTEGER PRIMARY KEY AUTOINCREMENT,
    version_name TEXT UNIQUE NOT NULL,  -- 'v1.0-bge-m3-base'
    embedding_model TEXT,               -- 'BGE-M3', 'mpnet', etc.
    matching_algorithm TEXT,            -- 'cosine-similarity', 'semantic-search'
    threshold REAL,                     -- Umbral de confianza m√≠nimo
    is_active INTEGER DEFAULT 0,
    created_at TEXT DEFAULT (datetime('now'))
);
```

#### CAPA 4: AUDITOR√çA Y ANALYTICS (2 tablas nuevas)

```sql
-- =====================================================================
-- TABLA: ofertas_historial
-- Tracking de TODOS los cambios en ofertas
-- =====================================================================
CREATE TABLE ofertas_historial (
    id INTEGER PRIMARY KEY AUTOINCREMENT,
    oferta_id TEXT NOT NULL,
    campo_modificado TEXT NOT NULL,    -- 'titulo', 'descripcion', 'salario_min', etc.
    valor_anterior TEXT,
    valor_nuevo TEXT,
    fecha_modificacion TEXT DEFAULT (datetime('now')),
    changed_by TEXT DEFAULT 'system',  -- 'scraper', 'nlp', 'manual'

    FOREIGN KEY (oferta_id) REFERENCES ofertas_v2(id_oferta) ON DELETE CASCADE
);

CREATE INDEX idx_historial_oferta ON ofertas_historial(oferta_id);
CREATE INDEX idx_historial_campo ON ofertas_historial(campo_modificado);
CREATE INDEX idx_historial_fecha ON ofertas_historial(fecha_modificacion);

-- =====================================================================
-- TABLA: analytics_cache
-- Cache de queries pesadas para dashboards
-- =====================================================================
CREATE TABLE analytics_cache (
    id INTEGER PRIMARY KEY AUTOINCREMENT,
    query_key TEXT UNIQUE NOT NULL,    -- 'top_skills_last_30_days'
    query_result TEXT NOT NULL,        -- JSON con resultado
    last_updated TEXT DEFAULT (datetime('now')),
    expires_at TEXT,                   -- TTL para invalidaci√≥n
    computation_time_ms INTEGER
);

CREATE INDEX idx_cache_key ON analytics_cache(query_key);
CREATE INDEX idx_cache_expires ON analytics_cache(expires_at);
```

### 3.3 Resumen de Tablas v2.0

**Total tablas v2.0**: 30 (22 v1 se mantienen + 8 nuevas)

| Categor√≠a | Tablas | Estado |
|-----------|--------|--------|
| **v1 Core (mantener para scheduler)** | ofertas, ofertas_nlp (38 cols) | MANTENER sin modificar |
| **v2 Scraping** | scraping_sessions, ofertas_raw, ofertas_v2, empresas | NUEVAS |
| **v2 NLP** | nlp_versions, ofertas_nlp_v2, skills, ofertas_skills, soft_skills, ofertas_soft_skills | NUEVAS |
| **ESCO (existentes)** | 17 tablas ESCO | MANTENER + fix bug skills |
| **Auditor√≠a** | ofertas_historial, analytics_cache | NUEVAS |

---

## 4. ESTRATEGIA DE MIGRACI√ìN

### 4.1 Principio: DUAL-WRITE (No Romper Producci√≥n)

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ          Scheduler (Lun/Jue 8:00 AM)        ‚îÇ
‚îÇ             run_scheduler.py                 ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                  ‚Üì
         ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
         ‚îÇ  db_manager.py ‚îÇ  ‚Üê MODIFICAR AQU√ç
         ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                  ‚Üì
    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
    ‚Üì                           ‚Üì
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê              ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ WRITE V1‚îÇ (CRITICAL)   ‚îÇ  WRITE V2   ‚îÇ (best effort)
‚îÇ ofertas ‚îÇ              ‚îÇ ofertas_v2  ‚îÇ
‚îÇ 38 cols ‚îÇ              ‚îÇ ofertas_raw ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò              ‚îÇscraping_sess‚îÇ
    ‚Üì                    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
    ‚Üì                           ‚Üì
[PRODUCCI√ìN]              [MIGRACI√ìN]
Scheduler sigue           Validaci√≥n en
funcionando               paralelo
```

### 4.2 Modificaci√≥n de db_manager.py

**Ubicaci√≥n**: `D:/OEDE/Webscrapping/database/db_manager.py`

```python
class DatabaseManager:
    """
    DUAL-WRITE: Escribe a v1 (producci√≥n) Y v2 (nueva arquitectura)
    """

    def __init__(self, db_path='bumeran_scraping.db'):
        self.conn = sqlite3.connect(db_path)
        self.v2_enabled = True  # Flag para habilitar/deshabilitar v2

    def insert_ofertas(self, ofertas_df, session_info=None):
        """
        Inserta ofertas en AMBOS schemas: v1 y v2

        Args:
            ofertas_df: DataFrame con ofertas scrapeadas
            session_info: Dict con info de scraping_session (para v2)

        Returns:
            (v1_count, v2_count): Tupla con registros insertados
        """
        cursor = self.conn.cursor()
        v1_inserted = 0
        v2_inserted = 0

        try:
            # ========================================
            # FASE 1: WRITE TO V1 (PRODUCCI√ìN)
            # SI ESTO FALLA ‚Üí RAISE ERROR
            # ========================================
            for _, row in ofertas_df.iterrows():
                # Convertir a formato v1 (38 columnas)
                v1_data = self._to_v1_format(row)

                cursor.execute("""
                    INSERT OR REPLACE INTO ofertas (
                        id_oferta, titulo, descripcion, ...  -- 38 columnas
                    ) VALUES (?, ?, ?, ...)
                """, v1_data)

                v1_inserted += 1

            self.conn.commit()
            logger.info(f"[V1] {v1_inserted} ofertas insertadas (PRODUCCI√ìN)")

            # ========================================
            # FASE 2: WRITE TO V2 (NUEVA ARQUITECTURA)
            # SI ESTO FALLA ‚Üí LOG WARNING, NO ROMPER
            # ========================================
            if self.v2_enabled:
                try:
                    # 2.1 Crear scraping_session
                    session_id = self._create_scraping_session(session_info)

                    for _, row in ofertas_df.iterrows():
                        # 2.2 ofertas_raw (immutable audit log)
                        self._insert_ofertas_raw(row, session_id)

                        # 2.3 ofertas_v2 (normalized)
                        self._insert_ofertas_v2(row)

                        # 2.4 empresas (si no existe)
                        if row.get('empresa'):
                            self._upsert_empresa(row['empresa'])

                        v2_inserted += 1

                    self.conn.commit()
                    logger.info(f"[V2] {v2_inserted} ofertas insertadas (MIGRACI√ìN)")

                except Exception as e:
                    logger.warning(f"[V2] Inserci√≥n fall√≥: {e}")
                    logger.warning("[V2] Continuando solo con v1...")
                    # NO raise - v1 ya tiene los datos

            return (v1_inserted, v2_inserted)

        except Exception as e:
            self.conn.rollback()
            logger.critical(f"[V1] Inserci√≥n CR√çTICA fall√≥: {e}")
            raise  # Esto rompe el scheduler ‚Üí necesario para no perder datos

    def _create_scraping_session(self, session_info):
        """Crea registro en scraping_sessions"""
        cursor = self.conn.cursor()

        cursor.execute("""
            INSERT INTO scraping_sessions (
                session_uuid, source, mode, start_time, keywords_used
            ) VALUES (?, ?, ?, ?, ?)
        """, (
            session_info.get('uuid'),
            session_info.get('source', 'bumeran'),
            session_info.get('mode', 'incremental'),
            session_info.get('start_time'),
            json.dumps(session_info.get('keywords', []))
        ))

        return cursor.lastrowid

    def _insert_ofertas_raw(self, row, session_id):
        """Inserta en ofertas_raw (audit log inmutable)"""
        import hashlib

        cursor = self.conn.cursor()

        # Calcular hash del contenido
        raw_json = row.to_json()
        content_hash = hashlib.sha256(raw_json.encode()).hexdigest()

        cursor.execute("""
            INSERT INTO ofertas_raw (
                id_oferta, scraping_session_id, raw_json, content_hash
            ) VALUES (?, ?, ?, ?)
        """, (
            row['id_oferta'],
            session_id,
            raw_json,
            content_hash
        ))

    def _insert_ofertas_v2(self, row):
        """Inserta en ofertas_v2 (normalizado)"""
        cursor = self.conn.cursor()

        # Obtener empresa_id
        empresa_id = None
        if row.get('empresa'):
            empresa_id = self._get_or_create_empresa(row['empresa'])

        cursor.execute("""
            INSERT OR REPLACE INTO ofertas_v2 (
                id_oferta, titulo, descripcion, empresa_id,
                provincia, localidad, area_trabajo,
                fecha_publicacion, source
            ) VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?)
        """, (
            row['id_oferta'],
            row['titulo'],
            row['descripcion'],
            empresa_id,
            row.get('provincia'),
            row.get('localidad'),
            row.get('area_trabajo'),
            row.get('fecha_publicacion'),
            'bumeran'
        ))
```

### 4.3 Constraints de Producci√≥n (NO ROMPER)

**Archivos CR√çTICOS que NO se pueden modificar**:

1. `run_scheduler.py` - NO TOCAR (scheduler production)
2. `bumeran_scraped_ids.json` - NO CORROMPER (12,847 IDs trackeados)
3. Tabla `ofertas` (38 columnas) - NO RENOMBRAR
4. Tabla `ofertas_nlp` - NO ELIMINAR
5. Primary key `id_oferta` - NO MODIFICAR tipo

**Lo que S√ç se puede modificar**:
- `db_manager.py` - Agregar dual-write
- `create_schema_v2.sql` - Aplicar (nuevas tablas, no afecta v1)
- Dashboard operativo - Agregar tabs de migraci√≥n

---

## 5. PIPELINE END-TO-END BUMERAN

### 5.1 Flujo Completo (Estado Objetivo)

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ FASE 1: SCRAPING                                                    ‚îÇ
‚îÇ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÇ
‚îÇ Scheduler ‚Üí Keywords ‚Üí Bumeran.com.ar ‚Üí Ofertas HTML               ‚îÇ
‚îÇ   ‚Üì                                                                 ‚îÇ
‚îÇ db_manager.py (DUAL-WRITE)                                          ‚îÇ
‚îÇ   ‚îú‚Üí V1: ofertas (38 cols)         [PRODUCCI√ìN - SCHEDULER]        ‚îÇ
‚îÇ   ‚îî‚Üí V2: ofertas_raw + ofertas_v2  [MIGRACI√ìN - VALIDACI√ìN]        ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                              ‚Üì
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ FASE 2: NLP EXTRACTION                                              ‚îÇ
‚îÇ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÇ
‚îÇ process_nlp_from_db_v4.py --mode hybrid                             ‚îÇ
‚îÇ   ‚îú‚Üí Regex v3.7 (fast, 70-80%)                                     ‚îÇ
‚îÇ   ‚îî‚Üí LLM v4.0 (solo campos vac√≠os, 95%+)                           ‚îÇ
‚îÇ                                                                     ‚îÇ
‚îÇ Destino:                                                            ‚îÇ
‚îÇ   ‚îú‚Üí V1: ofertas_nlp (sobrescribe)                                 ‚îÇ
‚îÇ   ‚îî‚Üí V2: ofertas_nlp_v2 (versioned)                                ‚îÇ
‚îÇ         ‚îú‚Üí skills ‚Üí Normalizadas en ofertas_skills                 ‚îÇ
‚îÇ         ‚îî‚Üí soft_skills ‚Üí ofertas_soft_skills                       ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                              ‚Üì
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ FASE 3: ESCO MATCHING                                               ‚îÇ
‚îÇ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÇ
‚îÇ [PRIMERO: FIX BUG] populate_esco_from_rdf.py                        ‚îÇ
‚îÇ   Fix SPARQL query ‚Üí 6 skills a 13,890 skills ‚úì                    ‚îÇ
‚îÇ                                                                     ‚îÇ
‚îÇ esco_hybrid_matcher.py                                              ‚îÇ
‚îÇ   ‚îú‚Üí BGE-M3 embeddings                                             ‚îÇ
‚îÇ   ‚îú‚Üí Cosine similarity                                             ‚îÇ
‚îÇ   ‚îî‚Üí Threshold 0.7                                                 ‚îÇ
‚îÇ                                                                     ‚îÇ
‚îÇ Destino: ofertas_esco_matching                                      ‚îÇ
‚îÇ   ‚îú‚Üí matched_occupation_uri                                        ‚îÇ
‚îÇ   ‚îú‚Üí matched_skills_uris                                           ‚îÇ
‚îÇ   ‚îî‚Üí confidence scores                                             ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                              ‚Üì
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ FASE 4: VISUALIZACI√ìN                                               ‚îÇ
‚îÇ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÇ
‚îÇ A. Dashboard Operativo (Dash - localhost:8050)                      ‚îÇ
‚îÇ    ‚îú‚Üí Tab: Calidad NLP (v1 vs v2 comparison)                       ‚îÇ
‚îÇ    ‚îú‚Üí Tab: ESCO Matching Coverage                                  ‚îÇ
‚îÇ    ‚îú‚Üí Tab: Migraci√≥n Status (v1 vs v2 sync)                        ‚îÇ
‚îÇ    ‚îî‚Üí Tab: Scraping Sessions                                       ‚îÇ
‚îÇ                                                                     ‚îÇ
‚îÇ B. Dashboard P√∫blico (Shiny - shinyapps.io)                         ‚îÇ
‚îÇ    ‚îú‚Üí An√°lisis de mercado laboral                                  ‚îÇ
‚îÇ    ‚îú‚Üí Skills m√°s demandadas                                        ‚îÇ
‚îÇ    ‚îú‚Üí Salarios por provincia                                       ‚îÇ
‚îÇ    ‚îî‚Üí Tendencias temporales                                        ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

### 5.2 Validaci√≥n de Cada Fase

#### Checkpoint 1: Post-Scraping

```sql
-- ¬øSe escribi√≥ en ambos schemas?
SELECT
    (SELECT COUNT(*) FROM ofertas) as v1_count,
    (SELECT COUNT(*) FROM ofertas_v2) as v2_count,
    (SELECT COUNT(*) FROM ofertas_raw) as raw_count;

-- Esperar: v1_count = v2_count = raw_count

-- ¬øScraping session creada?
SELECT * FROM scraping_sessions ORDER BY start_time DESC LIMIT 1;
```

#### Checkpoint 2: Post-NLP

```sql
-- ¬øVersiones NLP creadas?
SELECT version_name, COUNT(*) as ofertas_procesadas
FROM ofertas_nlp_v2
JOIN nlp_versions ON ofertas_nlp_v2.version_id = nlp_versions.id
GROUP BY version_name;

-- Esperar:
-- v3.7-regex-aggressive  | 5479
-- v4.0-hybrid-regex-llm  | 5479

-- ¬øSkills normalizadas?
SELECT COUNT(*) FROM skills;
SELECT COUNT(*) FROM ofertas_skills;

-- Esperar: skills > 200, ofertas_skills > 10000
```

#### Checkpoint 3: Post-ESCO

```sql
-- ¬øESCO skills corregidas?
SELECT COUNT(*) FROM esco_skills;
-- Esperar: ~13,890 (NO 6)

-- ¬øMatching coverage?
SELECT
    COUNT(DISTINCT oferta_id) * 100.0 / (SELECT COUNT(*) FROM ofertas_v2) as coverage
FROM ofertas_esco_matching;

-- Esperar: > 85%
```

---

## 6. FIXES CR√çTICOS

### 6.1 ESCO Skills Bug (BLOCKING)

**Archivo**: `D:/OEDE/Webscrapping/database/populate_esco_from_rdf.py`
**L√≠neas**: ~200-300 (secci√≥n de skills extraction)

**Problema**:
```sql
SELECT COUNT(*) FROM esco_skills;
-- Actual: 6
-- Esperado: ~13,890
```

**Fix requerido**:

```python
# populate_esco_from_rdf.py - ANTES (Bug)

def extract_skills(self, graph):
    """BUG: Query SPARQL incompleto"""
    query = """
        PREFIX esco: <http://data.europa.eu/esco/model#>
        SELECT ?skill ?label
        WHERE {
            ?skill a esco:Skill .
            ?skill skos:prefLabel ?label .
            FILTER(lang(?label) = "en")
        }
        LIMIT 10  # ‚Üê BUG: Solo 10 registros
    """

    results = graph.query(query)
    # Solo procesa 10, pero luego algo m√°s falla y solo quedan 6

# populate_esco_from_rdf.py - DESPU√âS (Fix)

def extract_skills(self, graph):
    """FIX: Query completa sin LIMIT"""
    query = """
        PREFIX esco: <http://data.europa.eu/esco/model#>
        PREFIX skos: <http://www.w3.org/2004/02/skos/core#>

        SELECT DISTINCT ?skill ?label_en ?label_es ?description
        WHERE {
            # Skill concept
            ?skill a esco:Skill .

            # English label (siempre presente)
            ?skill skos:prefLabel ?label_en .
            FILTER(lang(?label_en) = "en")

            # Spanish label (opcional)
            OPTIONAL {
                ?skill skos:prefLabel ?label_es .
                FILTER(lang(?label_es) = "es")
            }

            # Description (opcional)
            OPTIONAL {
                ?skill skos:definition ?description .
                FILTER(lang(?description) = "en")
            }
        }
        # NO LIMIT - queremos TODAS las skills
    """

    results = graph.query(query)
    skills_inserted = 0

    for row in results:
        skill_uri = str(row.skill)
        label_en = str(row.label_en)
        label_es = str(row.label_es) if row.label_es else label_en
        description = str(row.description) if row.description else None

        # INSERT into esco_skills
        cursor.execute("""
            INSERT OR IGNORE INTO esco_skills (
                skill_uri, preferred_label_en, preferred_label_es, description
            ) VALUES (?, ?, ?, ?)
        """, (skill_uri, label_en, label_es, description))

        skills_inserted += 1

    print(f"[OK] {skills_inserted} skills insertadas")
    return skills_inserted
```

**Validaci√≥n**:
```bash
# Re-ejecutar poblaci√≥n ESCO
cd D:/OEDE/Webscrapping/database
python populate_esco_from_rdf.py --force-reload

# Verificar
sqlite3 bumeran_scraping.db "SELECT COUNT(*) FROM esco_skills;"
# Esperar: ~13,890
```

### 6.2 Encoding Issues

**Problema**: Caracteres mal encodificados (√É¬≥, √É¬±, √¢‚Ç¨)

**Fix**: Aplicar en scraping antes de DB insert

```python
# D:/OEDE/Webscrapping/01_sources/bumeran/scrapers/scrapear_con_diccionario.py

class BumeranMultiSearch:

    def _clean_text(self, text):
        """Fix encoding antes de guardar en DB"""
        if not text:
            return text

        # Fix common encoding issues
        replacements = {
            '√É¬≥': '√≥',
            '√É¬°': '√°',
            '√É¬©': '√©',
            '√É¬≠': '√≠',
            '√É¬±': '√±',
            '√É¬∫': '√∫',
            '√¢‚Ç¨': '"',
            '√¢‚Ç¨‚Ñ¢': "'",
        }

        for bad, good in replacements.items():
            text = text.replace(bad, good)

        # Ensure UTF-8
        if isinstance(text, bytes):
            text = text.decode('utf-8', errors='replace')

        return text
```

---

## 7. TIMELINE DE IMPLEMENTACI√ìN

### AHORA (Preparaci√≥n - 2-3 horas)

#### ‚úÖ COMPLETADO
- [x] Crear directorio `database/migrations/`
- [x] Crear `backup_db.py`
- [x] Crear `create_schema_v2.sql`
- [x] Crear `test_data_integrity.py`
- [x] Crear `MIGRATION_PLAN.md` (este documento)

#### üîÑ EN PROCESO (Ejecutar AHORA)

**Hora 1: Backup y Tests** (30 min)
```bash
# 1.1 Backup de producci√≥n
cd D:/OEDE/Webscrapping/database/migrations
python backup_db.py --description "Pre-migraci√≥n v2.0"

# 1.2 Tests de integridad
python test_data_integrity.py --report integrity_pre_migration.json

# 1.3 Revisar resultados
# Si FAIL > 0 ‚Üí Resolver antes de continuar
```

**Hora 2: Aplicar Schema v2** (30 min)
```bash
# 2.1 Crear schema v2 (no afecta v1)
cd D:/OEDE/Webscrapping/database
sqlite3 bumeran_scraping.db < migrations/create_schema_v2.sql

# 2.2 Verificar tablas creadas
sqlite3 bumeran_scraping.db "SELECT name FROM sqlite_master WHERE type='table' ORDER BY name;"

# Esperar ver:
# - scraping_sessions
# - ofertas_raw
# - ofertas_v2
# - nlp_versions
# - ofertas_nlp_v2
# - skills
# - ofertas_skills
# - ...
```

**Hora 3: Fix ESCO Skills Bug** (60 min)
```bash
# 3.1 Backup de tabla esco_skills actual (solo 6 registros)
sqlite3 bumeran_scraping.db "CREATE TABLE esco_skills_backup AS SELECT * FROM esco_skills;"

# 3.2 Editar populate_esco_from_rdf.py (fix SPARQL query)
# Ver secci√≥n 6.1 de este plan

# 3.3 Re-ejecutar poblaci√≥n ESCO
python populate_esco_from_rdf.py --force-reload

# 3.4 Verificar fix
sqlite3 bumeran_scraping.db "SELECT COUNT(*) FROM esco_skills;"
# Esperar: ~13,890
```

### HOY (Implementaci√≥n Core - 4-6 horas)

**Hora 4-5: Modificar db_manager.py para Dual-Write** (90 min)
```bash
# 4.1 Backup de db_manager.py actual
cp database/db_manager.py database/db_manager_v1_backup.py

# 4.2 Implementar dual-write
# Ver secci√≥n 4.2 de este plan

# 4.3 Test con scraping manual (NO scheduler)
python -c "
from database.db_manager import DatabaseManager
import pandas as pd

# Test oferta
test_df = pd.DataFrame([{
    'id_oferta': 'TEST_001',
    'titulo': 'Test Dual Write',
    'descripcion': 'Testing...',
    'empresa': 'Test SA'
}])

db = DatabaseManager()
v1, v2 = db.insert_ofertas(test_df, {
    'uuid': 'test-session',
    'source': 'bumeran',
    'mode': 'manual',
    'start_time': '2025-11-02 00:00:00'
})

print(f'V1 insertados: {v1}')
print(f'V2 insertados: {v2}')
"

# 4.4 Verificar en DB
sqlite3 bumeran_scraping.db "
SELECT 'V1', COUNT(*) FROM ofertas WHERE id_oferta = 'TEST_001'
UNION ALL
SELECT 'V2', COUNT(*) FROM ofertas_v2 WHERE id_oferta = 'TEST_001';
"
```

**Hora 6-7: Migrar Datos Hist√≥ricos a v2** (120 min)
```python
# migrations/migrate_historical_data.py

import sqlite3
from tqdm import tqdm
import json

def migrate_ofertas_to_v2():
    """Migra todas las ofertas existentes de v1 a v2"""
    conn = sqlite3.connect('database/bumeran_scraping.db')
    cursor = conn.cursor()

    # 1. Crear session para migraci√≥n hist√≥rica
    cursor.execute("""
        INSERT INTO scraping_sessions (
            session_uuid, source, mode, start_time, status
        ) VALUES (?, ?, ?, ?, ?)
    """, ('migration-historical', 'bumeran', 'full',
          '2025-11-02 00:00:00', 'completed'))

    migration_session_id = cursor.lastrowid

    # 2. Obtener todas las ofertas v1
    cursor.execute("SELECT * FROM ofertas")
    ofertas_v1 = cursor.fetchall()
    columns = [desc[0] for desc in cursor.description]

    print(f"Migrando {len(ofertas_v1):,} ofertas a v2...")

    for row in tqdm(ofertas_v1):
        oferta = dict(zip(columns, row))

        # 2.1 ofertas_raw
        raw_json = json.dumps(oferta, ensure_ascii=False)
        content_hash = hashlib.sha256(raw_json.encode()).hexdigest()

        cursor.execute("""
            INSERT INTO ofertas_raw (
                id_oferta, scraping_session_id, raw_json, content_hash
            ) VALUES (?, ?, ?, ?)
        """, (oferta['id_oferta'], migration_session_id, raw_json, content_hash))

        # 2.2 ofertas_v2
        # Obtener/crear empresa
        empresa_id = None
        if oferta.get('empresa'):
            cursor.execute("SELECT id FROM empresas WHERE nombre = ?", (oferta['empresa'],))
            result = cursor.fetchone()
            if result:
                empresa_id = result[0]
            else:
                cursor.execute("INSERT INTO empresas (nombre) VALUES (?)", (oferta['empresa'],))
                empresa_id = cursor.lastrowid

        cursor.execute("""
            INSERT INTO ofertas_v2 (
                id_oferta, titulo, descripcion, empresa_id,
                provincia, localidad, area_trabajo,
                fecha_publicacion, source
            ) VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?)
        """, (
            oferta['id_oferta'],
            oferta['titulo'],
            oferta['descripcion'],
            empresa_id,
            oferta.get('provincia'),
            oferta.get('localidad'),
            oferta.get('area_trabajo'),
            oferta.get('fecha_publicacion'),
            'bumeran'
        ))

    conn.commit()
    print(f"[OK] {len(ofertas_v1):,} ofertas migradas a v2")

def migrate_nlp_to_v2():
    """Migra ofertas_nlp a ofertas_nlp_v2 con versionado"""
    conn = sqlite3.connect('database/bumeran_scraping.db')
    cursor = conn.cursor()

    # Obtener version_id actual
    cursor.execute("SELECT id FROM nlp_versions WHERE is_active = 1")
    active_version_id = cursor.fetchone()[0]

    # Migrar datos NLP
    cursor.execute("SELECT * FROM ofertas_nlp")
    ofertas_nlp = cursor.fetchall()
    columns = [desc[0] for desc in cursor.description]

    print(f"Migrando {len(ofertas_nlp):,} registros NLP a v2...")

    for row in tqdm(ofertas_nlp):
        nlp = dict(zip(columns, row))

        # Insertar en ofertas_nlp_v2
        cursor.execute("""
            INSERT INTO ofertas_nlp_v2 (
                id_oferta, version_id,
                experiencia_min_anios, experiencia_max_anios,
                nivel_educativo, estado_educativo, carrera_especifica,
                idioma_principal, nivel_idioma_principal,
                salario_min, salario_max, moneda,
                jornada_laboral, horario_flexible,
                confidence_score
            ) VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)
        """, (
            nlp['id_oferta'], active_version_id,
            nlp.get('experiencia_min_anios'),
            nlp.get('experiencia_max_anios'),
            nlp.get('nivel_educativo'),
            nlp.get('estado_educativo'),
            nlp.get('carrera_especifica'),
            nlp.get('idioma_principal'),
            nlp.get('nivel_idioma_principal'),
            nlp.get('salario_min'),
            nlp.get('salario_max'),
            nlp.get('moneda'),
            nlp.get('jornada_laboral'),
            nlp.get('horario_flexible'),
            nlp.get('nlp_confidence_score', 0.0)
        ))

        # Normalizar skills
        if nlp.get('skills_tecnicas_list'):
            try:
                skills = json.loads(nlp['skills_tecnicas_list'])
                for skill_name in skills:
                    # Get or create skill
                    cursor.execute("SELECT id FROM skills WHERE nombre = ?", (skill_name,))
                    result = cursor.fetchone()
                    if result:
                        skill_id = result[0]
                    else:
                        cursor.execute(
                            "INSERT INTO skills (nombre, tipo) VALUES (?, ?)",
                            (skill_name, 'tecnica')
                        )
                        skill_id = cursor.lastrowid

                    # Link oferta ‚Üî skill
                    cursor.execute("""
                        INSERT OR IGNORE INTO ofertas_skills (
                            oferta_id, skill_id, version_id
                        ) VALUES (?, ?, ?)
                    """, (nlp['id_oferta'], skill_id, active_version_id))
            except:
                pass

    conn.commit()
    print(f"[OK] {len(ofertas_nlp):,} registros NLP migrados")

if __name__ == '__main__':
    import hashlib
    migrate_ofertas_to_v2()
    migrate_nlp_to_v2()
```

**Ejecutar migraci√≥n**:
```bash
python migrations/migrate_historical_data.py
```

### MA√ëANA (Validaci√≥n - 2-4 horas)

**Validar Dual-Write con Scheduler** (Test completo)
```bash
# 1. Ejecutar scheduler en modo test
python run_scheduler.py --test

# 2. Verificar escritura en ambos schemas
sqlite3 database/bumeran_scraping.db "
SELECT
    'V1 ofertas' as tabla, COUNT(*) as count FROM ofertas
UNION ALL
SELECT 'V2 ofertas_v2', COUNT(*) FROM ofertas_v2
UNION ALL
SELECT 'V2 ofertas_raw', COUNT(*) FROM ofertas_raw;
"

# 3. Verificar √∫ltima session
sqlite3 database/bumeran_scraping.db "
SELECT * FROM scraping_sessions
ORDER BY start_time DESC LIMIT 1;
"
```

**Actualizar Dashboard Operativo**
```python
# Agregar en dashboard_scraping_v4.py

# Tab: Migraci√≥n Status
@app.callback(...)
def update_migration_status():
    """Compara v1 vs v2 en tiempo real"""

    # Query ambos schemas
    v1_count = pd.read_sql("SELECT COUNT(*) as c FROM ofertas", conn)
    v2_count = pd.read_sql("SELECT COUNT(*) as c FROM ofertas_v2", conn)

    sync_percentage = (v2_count['c'][0] / v1_count['c'][0]) * 100

    return html.Div([
        html.H3("Estado de Migraci√≥n v1 ‚Üí v2"),
        dcc.Graph(figure={
            'data': [{
                'x': ['V1 (Producci√≥n)', 'V2 (Nueva)'],
                'y': [v1_count['c'][0], v2_count['c'][0]],
                'type': 'bar'
            }],
            'layout': {
                'title': f'Sincronizaci√≥n: {sync_percentage:.1f}%'
            }
        })
    ])
```

### PR√ìXIMA SEMANA (Cutover - Planificado)

**Lunes**: Continuar validaci√≥n dual-write
**Martes**: Re-ejecutar NLP v4.0 con nueva arquitectura
**Mi√©rcoles**: Re-ejecutar ESCO matching con skills corregidas
**Jueves**: Validar scheduler production con v2 activo
**Viernes**: Actualizar Shiny dashboard con datos enriquecidos

---

## 8. DASHBOARD INTEGRATION

### 8.1 Dashboard Operativo (localhost:8050)

**Archivo**: `D:/OEDE/Webscrapping/dashboard_scraping_v4.py`

**Nuevos Tabs a Agregar**:

#### Tab: Migraci√≥n v1 ‚Üí v2

```python
# M√©tricas de sincronizaci√≥n
- Total ofertas v1 vs v2
- √öltima session de migraci√≥n
- Ofertas pendientes de migrar
- Tiempo de sync promedio
```

#### Tab: NLP Versions Comparison

```python
# Comparar versiones NLP
- v3.7 (regex) vs v4.0 (hybrid)
- Confidence score por versi√≥n
- Campos extra√≠dos por versi√≥n
- Tabla comparativa lado a lado
```

#### Tab: ESCO Integration Health

```python
# M√©tricas ESCO
- Total skills en ESCO: 13,890 ‚úì (antes: 6 ‚ùå)
- Cobertura de matching: 85%+
- Top 20 occupations detectadas
- Skills gap analysis
```

### 8.2 Dashboard P√∫blico (shinyapps.io)

**Archivo**: `D:/OEDE/Webscrapping/Visual--/app.R`

**Datos a actualizar**:
- Usar datos de `ofertas_v2` (normalizado)
- Usar `ofertas_esco_matching` (con fix de skills)
- Agregar filtro por "Skills ESCO" (ahora tenemos 13K+)

---

## 9. CRITERIOS DE VALIDACI√ìN

### 9.1 Validaci√≥n de Migraci√≥n Exitosa

#### ‚úÖ Criterio 1: Integridad de Datos

```sql
-- Test 1: Sin p√©rdida de datos
SELECT
    (SELECT COUNT(*) FROM ofertas) as v1,
    (SELECT COUNT(*) FROM ofertas_v2) as v2,
    CASE
        WHEN v1 = v2 THEN 'PASS ‚úì'
        ELSE 'FAIL ‚úó'
    END as status;

-- Test 2: Skills normalizadas
SELECT COUNT(*) FROM skills WHERE tipo = 'tecnica';
-- Esperar: > 200

SELECT COUNT(*) FROM ofertas_skills;
-- Esperar: > 10,000

-- Test 3: ESCO skills corregidas
SELECT COUNT(*) FROM esco_skills;
-- Esperar: >= 13,000
```

#### ‚úÖ Criterio 2: Performance

```sql
-- Query v1 (lento - JSON)
-- No se puede hacer eficientemente

-- Query v2 (r√°pido - normalizado)
SELECT s.nombre, COUNT(*) as ofertas_count
FROM skills s
JOIN ofertas_skills os ON s.id = os.skill_id
WHERE s.tipo = 'tecnica'
GROUP BY s.nombre
ORDER BY ofertas_count DESC
LIMIT 20;

-- Debe ejecutar en < 100ms
```

#### ‚úÖ Criterio 3: Scheduler NO Roto

```bash
# Ejecutar scheduler
python run_scheduler.py --test

# Verificar:
# 1. Scraping exitoso (ofertas nuevas > 0)
# 2. DB v1 actualizada (scheduler sigue funcionando)
# 3. DB v2 sincronizada (dual-write funciona)
# 4. Sin errores CRITICAL en logs
```

### 9.2 M√©tricas de √âxito

| M√©trica | Objetivo | Actual | Status |
|---------|----------|--------|--------|
| **ESCO Skills** | 13,890 | 6 | ‚ùå ‚Üí Arreglar |
| **NLP Coverage** | > 95% | ? | Medir post-fix |
| **Matching Coverage** | > 85% | ? | Medir post-fix |
| **Dual-Write Success** | 100% | - | Implementar |
| **Migration Sync** | 100% | - | Validar |
| **Dashboard Updated** | S√≠ | No | Agregar tabs |
| **Scheduler Working** | S√≠ | S√≠ | ‚úÖ Mantener |

---

## 10. ROLLBACK PLAN

### En Caso de Fallo Cr√≠tico

#### Scenario A: Scheduler Roto

```bash
# 1. Revertir db_manager.py
cp database/db_manager_v1_backup.py database/db_manager.py

# 2. Restart scheduler
# El scheduler volver√° a funcionar con v1 (sin dual-write)

# 3. Investigar error en logs
tail -f logs/scheduler_*.log
```

#### Scenario B: Corrupci√≥n de Datos

```bash
# 1. Restaurar desde backup
cd database/migrations
python -c "
import shutil
from pathlib import Path

# Encontrar √∫ltimo backup
backups = sorted(Path('backups').glob('*_backup_*.db'), reverse=True)
latest = backups[0]

# Restaurar
shutil.copy(latest, '../bumeran_scraping.db')
print(f'Restaurado desde: {latest}')
"

# 2. Verificar integridad
python test_data_integrity.py
```

#### Scenario C: ESCO Population Falla

```sql
-- Restaurar tabla ESCO skills desde backup
DROP TABLE esco_skills;
ALTER TABLE esco_skills_backup RENAME TO esco_skills;

-- Volver a intentar con query corregida
```

---

## RESUMEN EJECUTIVO

### ¬øQu√© estamos haciendo?

Dise√±ando e implementando **Database v2.0** para soportar el sistema completo de inteligencia laboral sin parches constantes.

### ¬øPor qu√©?

- Skills en JSON ‚Üí No queryable
- NLP sin versioning ‚Üí Perdemos historial
- ESCO con 6 skills ‚Üí Matching in√∫til (deber√≠a tener 13,890)
- Sin tracking de cambios ‚Üí No an√°lisis temporal

### ¬øC√≥mo?

1. **DUAL-WRITE**: Escribir a v1 (producci√≥n) Y v2 (nueva) simult√°neamente
2. **NO ROMPER SCHEDULER**: Lunes/Jueves 8:00 AM sigue funcionando
3. **FIX ESCO BUG**: 6 ‚Üí 13,890 skills
4. **BUMERAN-FIRST**: Completar pipeline end-to-end con una fuente antes de agregar m√°s
5. **DASHBOARD-VISIBLE**: Todo monitoreado en localhost en tiempo real

### ¬øCu√°ndo?

- **AHORA**: Backup, tests, crear schema v2, fix ESCO bug
- **HOY**: Implementar dual-write, migrar datos hist√≥ricos
- **MA√ëANA**: Validar con scheduler, actualizar dashboard
- **PR√ìXIMA SEMANA**: Cutover completo a v2

### Estado Actual de Archivos

```
‚úÖ COMPLETADOS:
- database/migrations/backup_db.py
- database/migrations/create_schema_v2.sql
- database/migrations/test_data_integrity.py
- database/migrations/MIGRATION_PLAN.md (este documento)

üîÑ PENDIENTES:
- database/migrations/migrate_historical_data.py (crear)
- database/db_manager.py (modificar para dual-write)
- database/populate_esco_from_rdf.py (fix SPARQL query)
- dashboard_scraping_v4.py (agregar tabs de migraci√≥n)

‚ö†Ô∏è CR√çTICOS (NO TOCAR):
- run_scheduler.py
- data/tracking/bumeran_scraped_ids.json
- tabla ofertas (38 columnas)
```

---

**Documento de Referencia**: Cuando olvides algo, vuelve a este plan.
**Pr√≥xima Acci√≥n**: Ejecutar backups y tests de integridad (secci√≥n 7, Hora 1)

---

*Generado: 2025-11-02*
*Versi√≥n: 2.0.0*
*Autor: Sistema de Migraci√≥n Database v2*
